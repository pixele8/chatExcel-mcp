#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢çš„MCPå·¥å…·ç³»ç»Ÿæ€§åˆ†æå’Œä¼˜åŒ–æ£€æµ‹è„šæœ¬
åŸºäºé—®é¢˜1-5çš„ä¿®å¤ç»éªŒï¼Œå¯¹æ‰€æœ‰24ä¸ªå·¥å…·è¿›è¡Œç¨³å®šæ€§æ£€æµ‹
"""

import ast
import inspect
import importlib.util
import sys
import os
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
import pandas as pd
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

class MCPToolsAnalyzer:
    """MCPå·¥å…·ç³»ç»Ÿæ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.server_file = self.project_root / "server.py"
        self.tools_info = []
        self.problem_categories = {
            "ç¼–ç é—®é¢˜": [],
            "å®‰å…¨æ£€æŸ¥é—®é¢˜": [],
            "æ‰§è¡Œç¯å¢ƒé—®é¢˜": [],
            "å‚æ•°éªŒè¯é—®é¢˜": [],
            "é”™è¯¯å¤„ç†é—®é¢˜": [],
            "æ€§èƒ½ä¼˜åŒ–é—®é¢˜": [],
            "å…¼å®¹æ€§é—®é¢˜": []
        }
        
    def extract_tools_from_server(self) -> List[Dict[str, Any]]:
        """ä»server.pyä¸­æå–æ‰€æœ‰MCPå·¥å…·ä¿¡æ¯"""
        print("=== æå–MCPå·¥å…·ä¿¡æ¯ ===")
        
        with open(self.server_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # è§£æAST
        tree = ast.parse(content)
        
        tools = []
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # æ£€æŸ¥æ˜¯å¦æœ‰@mcp.tool()è£…é¥°å™¨
                has_mcp_decorator = any(
                    # æ£€æŸ¥ @mcp.tool() è°ƒç”¨å½¢å¼
                    (isinstance(decorator, ast.Call) and
                     isinstance(decorator.func, ast.Attribute) and
                     isinstance(decorator.func.value, ast.Name) and
                     decorator.func.value.id == 'mcp' and
                     decorator.func.attr == 'tool') or
                    # æ£€æŸ¥ @mcp.tool å±æ€§å½¢å¼
                    (isinstance(decorator, ast.Attribute) and
                     isinstance(decorator.value, ast.Name) and
                     decorator.value.id == 'mcp' and
                     decorator.attr == 'tool')
                    for decorator in node.decorator_list
                )
                
                if has_mcp_decorator:
                    tool_info = self._analyze_tool_function(node, content)
                    tools.append(tool_info)
                    
        print(f"âœ“ å‘ç° {len(tools)} ä¸ªMCPå·¥å…·")
        return tools
    
    def _analyze_tool_function(self, node: ast.FunctionDef, content: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå·¥å…·å‡½æ•°"""
        # è·å–å‡½æ•°åŸºæœ¬ä¿¡æ¯
        func_name = node.name
        docstring = ast.get_docstring(node) or "æ— æ–‡æ¡£"
        
        # åˆ†æå‚æ•°
        args = []
        for arg in node.args.args:
            arg_info = {
                "name": arg.arg,
                "annotation": ast.unparse(arg.annotation) if arg.annotation else "Any"
            }
            args.append(arg_info)
            
        # åˆ†æè¿”å›ç±»å‹
        return_type = ast.unparse(node.returns) if node.returns else "Any"
        
        # è·å–å‡½æ•°æºç è¡Œæ•°
        start_line = node.lineno
        end_line = node.end_lineno or start_line
        
        # åˆ†æå‡½æ•°å¤æ‚åº¦
        complexity = self._calculate_complexity(node)
        
        # æ£€æµ‹æ½œåœ¨é—®é¢˜
        potential_issues = self._detect_potential_issues(node, content)
        
        return {
            "name": func_name,
            "docstring": docstring,
            "args": args,
            "return_type": return_type,
            "start_line": start_line,
            "end_line": end_line,
            "complexity": complexity,
            "potential_issues": potential_issues,
            "category": self._categorize_tool(func_name, docstring)
        }
    
    def _calculate_complexity(self, node: ast.FunctionDef) -> Dict[str, int]:
        """è®¡ç®—å‡½æ•°å¤æ‚åº¦"""
        complexity = {
            "lines": (node.end_lineno or node.lineno) - node.lineno + 1,
            "branches": 0,
            "loops": 0,
            "try_blocks": 0,
            "nested_functions": 0
        }
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For)):
                complexity["branches" if isinstance(child, ast.If) else "loops"] += 1
            elif isinstance(child, ast.Try):
                complexity["try_blocks"] += 1
            elif isinstance(child, ast.FunctionDef) and child != node:
                complexity["nested_functions"] += 1
                
        return complexity
    
    def _detect_potential_issues(self, node: ast.FunctionDef, content: str) -> List[Dict[str, str]]:
        """æ£€æµ‹æ½œåœ¨é—®é¢˜"""
        issues = []
        func_source = ast.unparse(node)
        
        # 1. ç¼–ç é—®é¢˜æ£€æµ‹
        if "encoding" in func_source.lower() and "detect" not in func_source.lower():
            issues.append({
                "category": "ç¼–ç é—®é¢˜",
                "type": "ç¼ºå°‘ç¼–ç æ£€æµ‹",
                "description": "å‡½æ•°å¤„ç†æ–‡ä»¶ä½†å¯èƒ½ç¼ºå°‘æ™ºèƒ½ç¼–ç æ£€æµ‹æœºåˆ¶",
                "severity": "medium"
            })
            
        # 2. å®‰å…¨æ£€æŸ¥é—®é¢˜
        if "exec" in func_source or "eval" in func_source:
            if "SecureCodeExecutor" not in func_source:
                issues.append({
                    "category": "å®‰å…¨æ£€æŸ¥é—®é¢˜",
                    "type": "ä¸å®‰å…¨çš„ä»£ç æ‰§è¡Œ",
                    "description": "ä½¿ç”¨exec/evalä½†æœªä½¿ç”¨å®‰å…¨æ‰§è¡Œå™¨",
                    "severity": "high"
                })
                
        # 3. é”™è¯¯å¤„ç†æ£€æµ‹
        has_try_except = any(isinstance(child, ast.Try) for child in ast.walk(node))
        if not has_try_except and node.name not in ["read_metadata", "get_tool_info"]:
            issues.append({
                "category": "é”™è¯¯å¤„ç†é—®é¢˜",
                "type": "ç¼ºå°‘å¼‚å¸¸å¤„ç†",
                "description": "å‡½æ•°ç¼ºå°‘try-excepté”™è¯¯å¤„ç†æœºåˆ¶",
                "severity": "medium"
            })
            
        # 4. å‚æ•°éªŒè¯æ£€æµ‹
        if not any("validate" in ast.unparse(child).lower() 
                  for child in ast.walk(node) if isinstance(child, ast.Call)):
            if len(node.args.args) > 2:  # å¤šå‚æ•°å‡½æ•°éœ€è¦éªŒè¯
                issues.append({
                    "category": "å‚æ•°éªŒè¯é—®é¢˜",
                    "type": "ç¼ºå°‘å‚æ•°éªŒè¯",
                    "description": "å¤šå‚æ•°å‡½æ•°ç¼ºå°‘è¾“å…¥éªŒè¯",
                    "severity": "medium"
                })
                
        # 5. æ€§èƒ½é—®é¢˜æ£€æµ‹
        if "pandas" in func_source and "chunk" not in func_source.lower():
            if "read_excel" in node.name or "process" in node.name:
                issues.append({
                    "category": "æ€§èƒ½ä¼˜åŒ–é—®é¢˜",
                    "type": "ç¼ºå°‘åˆ†å—å¤„ç†",
                    "description": "å¤§æ–‡ä»¶å¤„ç†å¯èƒ½éœ€è¦åˆ†å—æœºåˆ¶",
                    "severity": "low"
                })
                
        return issues
    
    def _categorize_tool(self, func_name: str, docstring: str) -> str:
        """å·¥å…·åˆ†ç±»"""
        name_lower = func_name.lower()
        doc_lower = docstring.lower()
        
        if "read" in name_lower or "è¯»å–" in doc_lower:
            return "æ•°æ®è¯»å–"
        elif "write" in name_lower or "å†™å…¥" in doc_lower:
            return "æ•°æ®å†™å…¥"
        elif "chart" in name_lower or "å›¾è¡¨" in doc_lower:
            return "å›¾è¡¨ç”Ÿæˆ"
        elif "code" in name_lower or "ä»£ç " in doc_lower:
            return "ä»£ç æ‰§è¡Œ"
        elif "validate" in name_lower or "éªŒè¯" in doc_lower:
            return "æ•°æ®éªŒè¯"
        elif "formula" in name_lower or "å…¬å¼" in doc_lower:
            return "å…¬å¼å¤„ç†"
        elif "metadata" in name_lower or "å…ƒæ•°æ®" in doc_lower:
            return "å…ƒæ•°æ®åˆ†æ"
        elif "batch" in name_lower or "æ‰¹é‡" in doc_lower:
            return "æ‰¹é‡å¤„ç†"
        else:
            return "å…¶ä»–å·¥å…·"
    
    def generate_optimization_plan(self, tools: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ"""
        print("\n=== ç”Ÿæˆç³»ç»Ÿä¼˜åŒ–æ–¹æ¡ˆ ===")
        
        # ç»Ÿè®¡é—®é¢˜åˆ†å¸ƒ
        issue_stats = {category: 0 for category in self.problem_categories.keys()}
        high_priority_tools = []
        
        for tool in tools:
            for issue in tool["potential_issues"]:
                issue_stats[issue["category"]] += 1
                if issue["severity"] == "high":
                    high_priority_tools.append({
                        "tool": tool["name"],
                        "issue": issue
                    })
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡å·¥å…·
        category_stats = {}
        for tool in tools:
            category = tool["category"]
            if category not in category_stats:
                category_stats[category] = 0
            category_stats[category] += 1
            
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_plan = {
            "æ€»ä½“ç»Ÿè®¡": {
                "å·¥å…·æ€»æ•°": len(tools),
                "é—®é¢˜æ€»æ•°": sum(len(tool["potential_issues"]) for tool in tools),
                "é«˜ä¼˜å…ˆçº§é—®é¢˜": len(high_priority_tools),
                "ç±»åˆ«åˆ†å¸ƒ": category_stats
            },
            "é—®é¢˜åˆ†å¸ƒ": issue_stats,
            "é«˜ä¼˜å…ˆçº§ä¿®å¤": high_priority_tools,
            "ä¼˜åŒ–å»ºè®®": self._generate_specific_recommendations(tools, issue_stats)
        }
        
        return optimization_plan
    
    def _generate_specific_recommendations(self, tools: List[Dict[str, Any]], 
                                         issue_stats: Dict[str, int]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå…·ä½“ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºé—®é¢˜ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        if issue_stats["ç¼–ç é—®é¢˜"] > 0:
            recommendations.append({
                "ç±»åˆ«": "ç¼–ç é—®é¢˜",
                "ä¼˜å…ˆçº§": "é«˜",
                "å»ºè®®": "ç»Ÿä¸€é›†æˆæ™ºèƒ½ç¼–ç æ£€æµ‹æœºåˆ¶åˆ°æ‰€æœ‰æ–‡ä»¶å¤„ç†å·¥å…·",
                "å®æ–½æ–¹æ¡ˆ": [
                    "åœ¨æ‰€æœ‰æ–‡ä»¶è¯»å–å·¥å…·ä¸­é›†æˆdetect_file_encodingå‡½æ•°",
                    "æ·»åŠ å¤šç¼–ç å°è¯•æœºåˆ¶å’Œå›é€€ç­–ç•¥",
                    "ç»Ÿä¸€ç¼–ç é”™è¯¯å¤„ç†å’Œç”¨æˆ·æç¤º"
                ]
            })
            
        if issue_stats["å®‰å…¨æ£€æŸ¥é—®é¢˜"] > 0:
            recommendations.append({
                "ç±»åˆ«": "å®‰å…¨æ£€æŸ¥é—®é¢˜",
                "ä¼˜å…ˆçº§": "é«˜",
                "å»ºè®®": "å¼ºåŒ–æ‰€æœ‰ä»£ç æ‰§è¡Œå·¥å…·çš„å®‰å…¨æœºåˆ¶",
                "å®æ–½æ–¹æ¡ˆ": [
                    "ç¡®ä¿æ‰€æœ‰ä»£ç æ‰§è¡Œéƒ½ä½¿ç”¨SecureCodeExecutor",
                    "ä¼˜åŒ–ASTå®‰å…¨åˆ†æå™¨çš„å‡½æ•°ç™½åå•",
                    "æ·»åŠ æ›´ç»†ç²’åº¦çš„å®‰å…¨ç­–ç•¥é…ç½®"
                ]
            })
            
        if issue_stats["é”™è¯¯å¤„ç†é—®é¢˜"] > 5:
            recommendations.append({
                "ç±»åˆ«": "é”™è¯¯å¤„ç†é—®é¢˜",
                "ä¼˜å…ˆçº§": "ä¸­",
                "å»ºè®®": "æ ‡å‡†åŒ–é”™è¯¯å¤„ç†æœºåˆ¶",
                "å®æ–½æ–¹æ¡ˆ": [
                    "ä¸ºæ‰€æœ‰å·¥å…·æ·»åŠ ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†è£…é¥°å™¨",
                    "å®ç°åˆ†å±‚é”™è¯¯å¤„ç†å’Œè¯¦ç»†é”™è¯¯æŠ¥å‘Š",
                    "æ·»åŠ é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶"
                ]
            })
            
        if issue_stats["å‚æ•°éªŒè¯é—®é¢˜"] > 3:
            recommendations.append({
                "ç±»åˆ«": "å‚æ•°éªŒè¯é—®é¢˜",
                "ä¼˜å…ˆçº§": "ä¸­",
                "å»ºè®®": "å®ç°ç»Ÿä¸€çš„å‚æ•°éªŒè¯æ¡†æ¶",
                "å®æ–½æ–¹æ¡ˆ": [
                    "åˆ›å»ºå‚æ•°éªŒè¯è£…é¥°å™¨",
                    "æ·»åŠ ç±»å‹æ£€æŸ¥å’ŒèŒƒå›´éªŒè¯",
                    "å®ç°å‚æ•°è‡ªåŠ¨è½¬æ¢å’Œæ¸…ç†"
                ]
            })
            
        if issue_stats["æ€§èƒ½ä¼˜åŒ–é—®é¢˜"] > 2:
            recommendations.append({
                "ç±»åˆ«": "æ€§èƒ½ä¼˜åŒ–é—®é¢˜",
                "ä¼˜å…ˆçº§": "ä½",
                "å»ºè®®": "ä¼˜åŒ–å¤§æ•°æ®å¤„ç†æ€§èƒ½",
                "å®æ–½æ–¹æ¡ˆ": [
                    "ä¸ºå¤§æ–‡ä»¶å¤„ç†æ·»åŠ åˆ†å—æœºåˆ¶",
                    "å®ç°å†…å­˜ä½¿ç”¨ç›‘æ§å’Œä¼˜åŒ–",
                    "æ·»åŠ è¿›åº¦æŠ¥å‘Šå’Œå–æ¶ˆæœºåˆ¶"
                ]
            })
            
        return recommendations
    
    def run_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” å¼€å§‹MCPå·¥å…·ç³»ç»Ÿæ€§åˆ†æ...")
        
        try:
            # 1. æå–å·¥å…·ä¿¡æ¯
            tools = self.extract_tools_from_server()
            self.tools_info = tools
            
            # 2. ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ
            optimization_plan = self.generate_optimization_plan(tools)
            
            # 3. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report = {
                "åˆ†ææ—¶é—´": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
                "å·¥å…·è¯¦æƒ…": tools,
                "ä¼˜åŒ–æ–¹æ¡ˆ": optimization_plan,
                "åˆ†ææ€»ç»“": self._generate_summary(tools, optimization_plan)
            }
            
            return report
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()
            return {"error": str(e), "traceback": traceback.format_exc()}
    
    def _generate_summary(self, tools: List[Dict[str, Any]], 
                         optimization_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        total_issues = sum(len(tool["potential_issues"]) for tool in tools)
        high_severity_issues = sum(
            1 for tool in tools 
            for issue in tool["potential_issues"] 
            if issue["severity"] == "high"
        )
        
        return {
            "å·¥å…·å¥åº·åº¦è¯„åˆ†": max(0, 100 - (total_issues * 5) - (high_severity_issues * 10)),
            "å…³é”®å‘ç°": [
                f"å‘ç° {len(tools)} ä¸ªMCPå·¥å…·",
                f"æ£€æµ‹åˆ° {total_issues} ä¸ªæ½œåœ¨é—®é¢˜",
                f"å…¶ä¸­ {high_severity_issues} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜éœ€è¦ç«‹å³ä¿®å¤",
                f"å»ºè®®å®æ–½ {len(optimization_plan['ä¼˜åŒ–å»ºè®®'])} é¡¹ä¼˜åŒ–æªæ–½"
            ],
            "ä¸‹ä¸€æ­¥è¡ŒåŠ¨": [
                "ä¼˜å…ˆä¿®å¤é«˜ä¼˜å…ˆçº§å®‰å…¨é—®é¢˜",
                "ç»Ÿä¸€å®æ–½ç¼–ç æ£€æµ‹æœºåˆ¶",
                "æ ‡å‡†åŒ–é”™è¯¯å¤„ç†æµç¨‹",
                "å»ºç«‹æŒç»­ç›‘æ§æœºåˆ¶"
            ]
        }

def main():
    """ä¸»å‡½æ•°"""
    analyzer = MCPToolsAnalyzer()
    report = analyzer.run_analysis()
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "comprehensive_tools_analysis_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # æ‰“å°å…³é”®ä¿¡æ¯
    if "error" not in report:
        print("\n=== åˆ†ææ€»ç»“ ===")
        summary = report["åˆ†ææ€»ç»“"]
        print(f"å·¥å…·å¥åº·åº¦è¯„åˆ†: {summary['å·¥å…·å¥åº·åº¦è¯„åˆ†']}/100")
        print("\nå…³é”®å‘ç°:")
        for finding in summary["å…³é”®å‘ç°"]:
            print(f"  â€¢ {finding}")
        print("\nä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        for action in summary["ä¸‹ä¸€æ­¥è¡ŒåŠ¨"]:
            print(f"  â€¢ {action}")
    
    return report

if __name__ == "__main__":
    main()