#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®ç»“æ„ä¼˜åŒ–å·¥å…·
ç”¨äºæ¸…ç†å†—ä½™æ–‡ä»¶ã€ä¼˜åŒ–ç›®å½•ç»“æ„å’Œæå‡é¡¹ç›®ç»„ç»‡æ€§
"""

import json
import shutil
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime
import re

class StructureOptimizer:
    """é¡¹ç›®ç»“æ„ä¼˜åŒ–å™¨"""
    
    def __init__(self, project_root: str):
        """åˆå§‹åŒ–ç»“æ„ä¼˜åŒ–å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "backups" / "structure_optimization"
        
    def analyze_structure(self) -> Dict:
        """åˆ†æé¡¹ç›®ç»“æ„
        
        Returns:
            ç»“æ„åˆ†æç»“æœ
        """
        print("ğŸ“Š å¼€å§‹é¡¹ç›®ç»“æ„åˆ†æ...")
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "duplicate_files": self._find_duplicate_files(),
            "empty_directories": self._find_empty_directories(),
            "large_files": self._find_large_files(),
            "temporary_files": self._find_temporary_files(),
            "redundant_configs": self._find_redundant_configs(),
            "unused_assets": self._find_unused_assets(),
            "structure_suggestions": self._analyze_structure_patterns()
        }
        
        return analysis
    
    def _find_duplicate_files(self) -> List[Dict]:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶
        
        Returns:
            é‡å¤æ–‡ä»¶åˆ—è¡¨
        """
        print("ğŸ” æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
        
        file_hashes = {}
        duplicates = []
        
        # æ’é™¤çš„ç›®å½•
        exclude_dirs = {'.git', '__pycache__', 'node_modules', '.venv', 'venv', 'cache', 'logs'}
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file() and not any(part in exclude_dirs for part in file_path.parts):
                try:
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                    with open(file_path, 'rb') as f:
                        file_hash = hashlib.md5(f.read()).hexdigest()
                    
                    if file_hash in file_hashes:
                        # æ‰¾åˆ°é‡å¤æ–‡ä»¶
                        duplicates.append({
                            'hash': file_hash,
                            'original': str(file_hashes[file_hash].relative_to(self.project_root)),
                            'duplicate': str(file_path.relative_to(self.project_root)),
                            'size': file_path.stat().st_size
                        })
                    else:
                        file_hashes[file_hash] = file_path
                        
                except Exception:
                    continue
                    
        return duplicates
    
    def _find_empty_directories(self) -> List[str]:
        """æŸ¥æ‰¾ç©ºç›®å½•
        
        Returns:
            ç©ºç›®å½•åˆ—è¡¨
        """
        print("ğŸ“ æŸ¥æ‰¾ç©ºç›®å½•...")
        
        empty_dirs = []
        
        for dir_path in self.project_root.rglob('*'):
            if dir_path.is_dir():
                try:
                    # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©ºï¼ˆå¿½ç•¥éšè—æ–‡ä»¶ï¼‰
                    contents = [item for item in dir_path.iterdir() if not item.name.startswith('.')]
                    if not contents:
                        empty_dirs.append(str(dir_path.relative_to(self.project_root)))
                except Exception:
                    continue
                    
        return empty_dirs
    
    def _find_large_files(self, size_threshold: int = 10 * 1024 * 1024) -> List[Dict]:
        """æŸ¥æ‰¾å¤§æ–‡ä»¶
        
        Args:
            size_threshold: å¤§å°é˜ˆå€¼ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤10MB
            
        Returns:
            å¤§æ–‡ä»¶åˆ—è¡¨
        """
        print("ğŸ“ æŸ¥æ‰¾å¤§æ–‡ä»¶...")
        
        large_files = []
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                try:
                    size = file_path.stat().st_size
                    if size > size_threshold:
                        large_files.append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'size': size,
                            'size_mb': round(size / (1024 * 1024), 2),
                            'type': file_path.suffix or 'no_extension'
                        })
                except Exception:
                    continue
                    
        # æŒ‰å¤§å°æ’åº
        large_files.sort(key=lambda x: x['size'], reverse=True)
        return large_files
    
    def _find_temporary_files(self) -> List[str]:
        """æŸ¥æ‰¾ä¸´æ—¶æ–‡ä»¶
        
        Returns:
            ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
        """
        print("ğŸ—‘ï¸ æŸ¥æ‰¾ä¸´æ—¶æ–‡ä»¶...")
        
        temp_patterns = [
            r'.*\.tmp$',
            r'.*\.temp$',
            r'.*\.bak$',
            r'.*\.backup$',
            r'.*~$',
            r'.*\.swp$',
            r'.*\.swo$',
            r'.*\.log$',
            r'.*\.cache$',
            r'.*\.pid$',
            r'.*\.lock$',
            r'.*\.orig$',
            r'.*\.rej$',
            r'.*\.pyc$',
            r'.*\.pyo$',
            r'.*\.pyd$',
            r'.*__pycache__.*',
            r'.*\.DS_Store$',
            r'.*Thumbs\.db$',
            r'.*\.coverage$',
            r'.*\.pytest_cache.*',
            r'.*\.mypy_cache.*',
            r'.*\.tox.*',
            r'.*\.nox.*'
        ]
        
        temp_files = []
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                file_str = str(file_path.relative_to(self.project_root))
                for pattern in temp_patterns:
                    if re.match(pattern, file_str, re.IGNORECASE):
                        temp_files.append(file_str)
                        break
                        
        return temp_files
    
    def _find_redundant_configs(self) -> List[Dict]:
        """æŸ¥æ‰¾å†—ä½™é…ç½®æ–‡ä»¶
        
        Returns:
            å†—ä½™é…ç½®æ–‡ä»¶åˆ—è¡¨
        """
        print("âš™ï¸ æŸ¥æ‰¾å†—ä½™é…ç½®æ–‡ä»¶...")
        
        config_files = {
            'mcp_config': [],
            'server_config': [],
            'test_config': [],
            'requirements': []
        }
        
        # æŸ¥æ‰¾å„ç±»é…ç½®æ–‡ä»¶
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                name = file_path.name.lower()
                
                if 'mcp_config' in name and name.endswith('.json'):
                    config_files['mcp_config'].append(str(file_path.relative_to(self.project_root)))
                elif 'server' in name and (name.endswith('.py') or name.endswith('.json')):
                    config_files['server_config'].append(str(file_path.relative_to(self.project_root)))
                elif 'test' in name and name.endswith('.py'):
                    config_files['test_config'].append(str(file_path.relative_to(self.project_root)))
                elif 'requirements' in name and name.endswith('.txt'):
                    config_files['requirements'].append(str(file_path.relative_to(self.project_root)))
                    
        # è¯†åˆ«å†—ä½™
        redundant = []
        for config_type, files in config_files.items():
            if len(files) > 1:
                redundant.append({
                    'type': config_type,
                    'files': files,
                    'count': len(files),
                    'suggestion': self._get_config_consolidation_suggestion(config_type, files)
                })
                
        return redundant
    
    def _find_unused_assets(self) -> List[str]:
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„èµ„æºæ–‡ä»¶
        
        Returns:
            æœªä½¿ç”¨èµ„æºæ–‡ä»¶åˆ—è¡¨
        """
        print("ğŸ–¼ï¸ æŸ¥æ‰¾æœªä½¿ç”¨çš„èµ„æºæ–‡ä»¶...")
        
        # æŸ¥æ‰¾æ‰€æœ‰èµ„æºæ–‡ä»¶
        asset_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.css', '.js', '.html'}
        assets = []
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in asset_extensions:
                assets.append(file_path)
                
        # æŸ¥æ‰¾ä»£ç æ–‡ä»¶ä¸­çš„å¼•ç”¨
        referenced_assets = set()
        code_extensions = {'.py', '.html', '.css', '.js', '.md', '.json', '.yaml', '.yml'}
        
        for code_file in self.project_root.rglob('*'):
            if code_file.is_file() and code_file.suffix.lower() in code_extensions:
                try:
                    with open(code_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        
                    # æŸ¥æ‰¾èµ„æºæ–‡ä»¶å¼•ç”¨
                    for asset in assets:
                        asset_name = asset.name
                        if asset_name in content:
                            referenced_assets.add(asset)
                            
                except Exception:
                    continue
                    
        # æ‰¾å‡ºæœªå¼•ç”¨çš„èµ„æº
        unused_assets = []
        for asset in assets:
            if asset not in referenced_assets:
                # æ’é™¤ä¸€äº›ç‰¹æ®Šæ–‡ä»¶
                if asset.name not in ['favicon.ico', 'robots.txt', 'sitemap.xml']:
                    unused_assets.append(str(asset.relative_to(self.project_root)))
                    
        return unused_assets
    
    def _analyze_structure_patterns(self) -> List[Dict]:
        """åˆ†æç»“æ„æ¨¡å¼å¹¶æä¾›å»ºè®®
        
        Returns:
            ç»“æ„å»ºè®®åˆ—è¡¨
        """
        print("ğŸ—ï¸ åˆ†æé¡¹ç›®ç»“æ„æ¨¡å¼...")
        
        suggestions = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆé€‚çš„ç›®å½•ç»“æ„
        expected_dirs = ['src', 'tests', 'docs', 'config', 'scripts']
        missing_dirs = []
        
        for expected_dir in expected_dirs:
            if not (self.project_root / expected_dir).exists():
                missing_dirs.append(expected_dir)
                
        if missing_dirs:
            suggestions.append({
                'type': 'missing_directories',
                'description': 'ç¼ºå°‘æ ‡å‡†ç›®å½•ç»“æ„',
                'missing': missing_dirs,
                'suggestion': 'è€ƒè™‘åˆ›å»ºæ ‡å‡†çš„é¡¹ç›®ç›®å½•ç»“æ„'
            })
            
        # æ£€æŸ¥æ–‡ä»¶åˆ†å¸ƒ
        root_py_files = [f for f in self.project_root.glob('*.py') if f.is_file()]
        if len(root_py_files) > 5:
            suggestions.append({
                'type': 'too_many_root_files',
                'description': 'æ ¹ç›®å½•ä¸‹Pythonæ–‡ä»¶è¿‡å¤š',
                'count': len(root_py_files),
                'files': [f.name for f in root_py_files],
                'suggestion': 'è€ƒè™‘å°†Pythonæ–‡ä»¶ç§»åŠ¨åˆ°src/æˆ–é€‚å½“çš„å­ç›®å½•ä¸­'
            })
            
        # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶ç»„ç»‡
        test_files = list(self.project_root.rglob('test_*.py')) + list(self.project_root.rglob('*_test.py'))
        test_dir_files = list((self.project_root / 'tests').rglob('*.py')) if (self.project_root / 'tests').exists() else []
        test_dir_files += list((self.project_root / 'test').rglob('*.py')) if (self.project_root / 'test').exists() else []
        
        scattered_tests = [f for f in test_files if not any(part in ['tests', 'test'] for part in f.parts)]
        
        if scattered_tests:
            suggestions.append({
                'type': 'scattered_test_files',
                'description': 'æµ‹è¯•æ–‡ä»¶åˆ†æ•£åœ¨é¡¹ç›®ä¸­',
                'count': len(scattered_tests),
                'files': [str(f.relative_to(self.project_root)) for f in scattered_tests],
                'suggestion': 'è€ƒè™‘å°†æ‰€æœ‰æµ‹è¯•æ–‡ä»¶ç§»åŠ¨åˆ°tests/ç›®å½•ä¸­'
            })
            
        # æ£€æŸ¥é…ç½®æ–‡ä»¶ç»„ç»‡
        config_files = []
        for pattern in ['*.json', '*.yaml', '*.yml', '*.toml', '*.ini', '*.cfg']:
            config_files.extend(self.project_root.glob(pattern))
            
        root_config_files = [f for f in config_files if f.parent == self.project_root]
        
        if len(root_config_files) > 3:
            suggestions.append({
                'type': 'too_many_root_configs',
                'description': 'æ ¹ç›®å½•ä¸‹é…ç½®æ–‡ä»¶è¿‡å¤š',
                'count': len(root_config_files),
                'files': [f.name for f in root_config_files],
                'suggestion': 'è€ƒè™‘å°†é…ç½®æ–‡ä»¶ç§»åŠ¨åˆ°config/ç›®å½•ä¸­'
            })
            
        return suggestions
    
    def _get_config_consolidation_suggestion(self, config_type: str, files: List[str]) -> str:
        """è·å–é…ç½®æ–‡ä»¶æ•´åˆå»ºè®®
        
        Args:
            config_type: é…ç½®ç±»å‹
            files: æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            æ•´åˆå»ºè®®
        """
        suggestions = {
            'mcp_config': 'ä¿ç•™ä¸€ä¸ªä¸»è¦çš„MCPé…ç½®æ–‡ä»¶ï¼Œå°†å…¶ä»–é…ç½®åˆå¹¶æˆ–ç§»åŠ¨åˆ°config/ç›®å½•',
            'server_config': 'æ•´åˆæœåŠ¡å™¨é…ç½®ï¼Œä¿ç•™ä¸€ä¸ªä¸»æœåŠ¡å™¨æ–‡ä»¶',
            'test_config': 'å°†æµ‹è¯•æ–‡ä»¶ç§»åŠ¨åˆ°tests/ç›®å½•å¹¶æ•´åˆé‡å¤çš„æµ‹è¯•',
            'requirements': 'ä¿ç•™requirements.txtï¼Œå°†å…¶ä»–ä¾èµ–æ–‡ä»¶æ•´åˆåˆ°pyproject.toml'
        }
        
        return suggestions.get(config_type, 'è€ƒè™‘æ•´åˆé‡å¤çš„é…ç½®æ–‡ä»¶')
    
    def optimize_structure(self, analysis: Dict, auto_fix: bool = False) -> Dict:
        """ä¼˜åŒ–é¡¹ç›®ç»“æ„
        
        Args:
            analysis: ç»“æ„åˆ†æç»“æœ
            auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®å¤
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        print("ğŸ”§ å¼€å§‹ç»“æ„ä¼˜åŒ–...")
        
        optimization_results = {
            'timestamp': datetime.now().isoformat(),
            'actions_taken': [],
            'manual_actions_needed': [],
            'backup_created': False
        }
        
        if auto_fix:
            # åˆ›å»ºå¤‡ä»½
            self._create_backup()
            optimization_results['backup_created'] = True
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            for temp_file in analysis['temporary_files']:
                try:
                    file_path = self.project_root / temp_file
                    if file_path.exists():
                        file_path.unlink()
                        optimization_results['actions_taken'].append(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_file}: {e}")
                    
            # åˆ é™¤ç©ºç›®å½•
            for empty_dir in analysis['empty_directories']:
                try:
                    dir_path = self.project_root / empty_dir
                    if dir_path.exists() and dir_path.is_dir():
                        dir_path.rmdir()
                        optimization_results['actions_taken'].append(f"åˆ é™¤ç©ºç›®å½•: {empty_dir}")
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•åˆ é™¤ç©ºç›®å½• {empty_dir}: {e}")
                    
            # åˆ›å»ºå»ºè®®çš„ç›®å½•ç»“æ„
            for suggestion in analysis['structure_suggestions']:
                if suggestion['type'] == 'missing_directories':
                    for missing_dir in suggestion['missing']:
                        try:
                            dir_path = self.project_root / missing_dir
                            dir_path.mkdir(exist_ok=True)
                            optimization_results['actions_taken'].append(f"åˆ›å»ºç›®å½•: {missing_dir}")
                        except Exception as e:
                            print(f"âš ï¸ æ— æ³•åˆ›å»ºç›®å½• {missing_dir}: {e}")
                            
        # æ·»åŠ éœ€è¦æ‰‹åŠ¨å¤„ç†çš„é¡¹ç›®
        for duplicate in analysis['duplicate_files']:
            optimization_results['manual_actions_needed'].append(
                f"æ‰‹åŠ¨æ£€æŸ¥é‡å¤æ–‡ä»¶: {duplicate['original']} vs {duplicate['duplicate']}"
            )
            
        for large_file in analysis['large_files'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæœ€å¤§çš„æ–‡ä»¶
            optimization_results['manual_actions_needed'].append(
                f"æ£€æŸ¥å¤§æ–‡ä»¶: {large_file['file']} ({large_file['size_mb']} MB)"
            )
            
        for redundant in analysis['redundant_configs']:
            optimization_results['manual_actions_needed'].append(
                f"æ•´åˆå†—ä½™é…ç½®: {redundant['type']} - {redundant['suggestion']}"
            )
            
        return optimization_results
    
    def _create_backup(self):
        """åˆ›å»ºé¡¹ç›®å¤‡ä»½"""
        print("ğŸ’¾ åˆ›å»ºé¡¹ç›®å¤‡ä»½...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = self.backup_dir / f"backup_{timestamp}"
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_path.mkdir(parents=True, exist_ok=True)
        
        # å¤‡ä»½é‡è¦æ–‡ä»¶
        important_files = [
            'requirements.txt',
            'pyproject.toml',
            'README.md',
            'config/',
            'scripts/'
        ]
        
        for item in important_files:
            source = self.project_root / item
            if source.exists():
                dest = backup_path / item
                try:
                    if source.is_file():
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(source, dest)
                    elif source.is_dir():
                        shutil.copytree(source, dest, dirs_exist_ok=True)
                except Exception as e:
                    print(f"âš ï¸ å¤‡ä»½å¤±è´¥ {item}: {e}")
                    
        print(f"âœ… å¤‡ä»½å·²åˆ›å»º: {backup_path}")
    
    def generate_optimization_report(self, analysis: Dict, optimization_results: Dict = None) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        
        Args:
            analysis: åˆ†æç»“æœ
            optimization_results: ä¼˜åŒ–ç»“æœ
            
        Returns:
            ä¼˜åŒ–æŠ¥å‘Šå†…å®¹
        """
        report = f"""
# é¡¹ç›®ç»“æ„ä¼˜åŒ–æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {analysis['timestamp']}

## åˆ†ææ¦‚è§ˆ
- é‡å¤æ–‡ä»¶: {len(analysis['duplicate_files'])} ç»„
- ç©ºç›®å½•: {len(analysis['empty_directories'])} ä¸ª
- å¤§æ–‡ä»¶: {len(analysis['large_files'])} ä¸ª
- ä¸´æ—¶æ–‡ä»¶: {len(analysis['temporary_files'])} ä¸ª
- å†—ä½™é…ç½®: {len(analysis['redundant_configs'])} ç»„
- æœªä½¿ç”¨èµ„æº: {len(analysis['unused_assets'])} ä¸ª
- ç»“æ„å»ºè®®: {len(analysis['structure_suggestions'])} é¡¹

## é‡å¤æ–‡ä»¶è¯¦æƒ…
"""
        
        for duplicate in analysis['duplicate_files']:
            report += f"""
### é‡å¤æ–‡ä»¶ç»„ (å¤§å°: {duplicate['size']} å­—èŠ‚)
- åŸæ–‡ä»¶: {duplicate['original']}
- é‡å¤æ–‡ä»¶: {duplicate['duplicate']}

"""
            
        report += "\n## å¤§æ–‡ä»¶åˆ—è¡¨\n"
        
        for large_file in analysis['large_files'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            report += f"- {large_file['file']}: {large_file['size_mb']} MB\n"
            
        report += "\n## ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨\n"
        
        for temp_file in analysis['temporary_files'][:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            report += f"- {temp_file}\n"
            
        if len(analysis['temporary_files']) > 20:
            report += f"... è¿˜æœ‰ {len(analysis['temporary_files']) - 20} ä¸ªä¸´æ—¶æ–‡ä»¶\n"
            
        report += "\n## å†—ä½™é…ç½®è¯¦æƒ…\n"
        
        for redundant in analysis['redundant_configs']:
            report += f"""
### {redundant['type']} ({redundant['count']} ä¸ªæ–‡ä»¶)
æ–‡ä»¶åˆ—è¡¨:
"""
            for file in redundant['files']:
                report += f"- {file}\n"
            report += f"**å»ºè®®**: {redundant['suggestion']}\n\n"
            
        report += "\n## ç»“æ„ä¼˜åŒ–å»ºè®®\n"
        
        for suggestion in analysis['structure_suggestions']:
            report += f"""
### {suggestion['description']}
- **ç±»å‹**: {suggestion['type']}
- **å»ºè®®**: {suggestion['suggestion']}

"""
            
        if optimization_results:
            report += f"""

## ä¼˜åŒ–æ‰§è¡Œç»“æœ

æ‰§è¡Œæ—¶é—´: {optimization_results['timestamp']}
å¤‡ä»½åˆ›å»º: {'æ˜¯' if optimization_results['backup_created'] else 'å¦'}

### å·²æ‰§è¡Œçš„æ“ä½œ
"""
            for action in optimization_results['actions_taken']:
                report += f"- {action}\n"
                
            report += "\n### éœ€è¦æ‰‹åŠ¨å¤„ç†çš„é¡¹ç›®\n"
            
            for action in optimization_results['manual_actions_needed']:
                report += f"- {action}\n"
                
        return report
    
    def save_optimization_report(self, analysis: Dict, optimization_results: Dict = None, output_file: str = None):
        """ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š
        
        Args:
            analysis: åˆ†æç»“æœ
            optimization_results: ä¼˜åŒ–ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = self.project_root / "structure_optimization_report.md"
            
        report = self.generate_optimization_report(analysis, optimization_results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(f"ğŸ“Š ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # åŒæ—¶ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_data = {
            'analysis': analysis,
            'optimization_results': optimization_results
        }
        
        json_file = str(output_file).replace('.md', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ“Š è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {json_file}")

def main():
    """ä¸»å‡½æ•°"""
    project_root = Path.cwd()
    
    print("ğŸš€ å¼€å§‹é¡¹ç›®ç»“æ„ä¼˜åŒ–...")
    optimizer = StructureOptimizer(str(project_root))
    
    # æ‰§è¡Œåˆ†æ
    analysis = optimizer.analyze_structure()
    
    # æ‰§è¡Œä¼˜åŒ–ï¼ˆè‡ªåŠ¨ä¿®å¤å®‰å…¨çš„æ“ä½œï¼‰
    optimization_results = optimizer.optimize_structure(analysis, auto_fix=True)
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    optimizer.save_optimization_report(analysis, optimization_results)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š ç»“æ„åˆ†ææ‘˜è¦:")
    print(f"- é‡å¤æ–‡ä»¶: {len(analysis['duplicate_files'])} ç»„")
    print(f"- ç©ºç›®å½•: {len(analysis['empty_directories'])} ä¸ª")
    print(f"- å¤§æ–‡ä»¶: {len(analysis['large_files'])} ä¸ª")
    print(f"- ä¸´æ—¶æ–‡ä»¶: {len(analysis['temporary_files'])} ä¸ª")
    print(f"- å†—ä½™é…ç½®: {len(analysis['redundant_configs'])} ç»„")
    print(f"- æœªä½¿ç”¨èµ„æº: {len(analysis['unused_assets'])} ä¸ª")
    
    print("\nğŸ”§ ä¼˜åŒ–æ‰§è¡Œæ‘˜è¦:")
    print(f"- å·²æ‰§è¡Œæ“ä½œ: {len(optimization_results['actions_taken'])} é¡¹")
    print(f"- éœ€æ‰‹åŠ¨å¤„ç†: {len(optimization_results['manual_actions_needed'])} é¡¹")
    print(f"- å¤‡ä»½å·²åˆ›å»º: {'æ˜¯' if optimization_results['backup_created'] else 'å¦'}")
    
if __name__ == "__main__":
    main()