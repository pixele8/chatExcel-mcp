#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é…ç½®ä¼˜åŒ–å™¨
ç»Ÿä¸€ç®¡ç†å’Œä¼˜åŒ–é¡¹ç›®çš„å„ç§é…ç½®æ–‡ä»¶
"""

import json
import toml
import yaml
import configparser
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import shutil
import re

class ConfigOptimizer:
    """é…ç½®ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self, project_root: str):
        """åˆå§‹åŒ–é…ç½®ä¼˜åŒ–å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "config_backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # é…ç½®æ–‡ä»¶æ¨¡å¼
        self.config_patterns = {
            'python': {
                'pyproject.toml': 'toml',
                'setup.py': 'python',
                'setup.cfg': 'ini',
                'requirements.txt': 'text',
                'requirements-dev.txt': 'text',
                'Pipfile': 'toml',
                'poetry.lock': 'toml',
                'uv.lock': 'toml'
            },
            'javascript': {
                'package.json': 'json',
                'package-lock.json': 'json',
                'yarn.lock': 'text',
                'tsconfig.json': 'json',
                '.eslintrc.json': 'json',
                '.prettierrc': 'json'
            },
            'docker': {
                'Dockerfile': 'text',
                'docker-compose.yml': 'yaml',
                'docker-compose.yaml': 'yaml',
                '.dockerignore': 'text'
            },
            'ci_cd': {
                '.github/workflows/*.yml': 'yaml',
                '.github/workflows/*.yaml': 'yaml',
                '.gitlab-ci.yml': 'yaml',
                'Jenkinsfile': 'text'
            },
            'editor': {
                '.vscode/settings.json': 'json',
                '.vscode/launch.json': 'json',
                '.editorconfig': 'ini'
            },
            'git': {
                '.gitignore': 'text',
                '.gitattributes': 'text'
            },
            'other': {
                'Makefile': 'text',
                'README.md': 'text',
                'LICENSE': 'text'
            }
        }
        
        # ä¼˜åŒ–è§„åˆ™
        self.optimization_rules = {
            'pyproject.toml': self._optimize_pyproject_toml,
            'requirements.txt': self._optimize_requirements_txt,
            'package.json': self._optimize_package_json,
            '.gitignore': self._optimize_gitignore,
            'docker-compose.yml': self._optimize_docker_compose
        }
        
    def scan_config_files(self) -> Dict[str, List[Path]]:
        """æ‰«æé¡¹ç›®ä¸­çš„é…ç½®æ–‡ä»¶
        
        Returns:
            æŒ‰ç±»åˆ«åˆ†ç»„çš„é…ç½®æ–‡ä»¶åˆ—è¡¨
        """
        found_configs = {}
        
        for category, patterns in self.config_patterns.items():
            found_configs[category] = []
            
            for pattern, file_type in patterns.items():
                if '*' in pattern:
                    # é€šé…ç¬¦æ¨¡å¼
                    for file_path in self.project_root.rglob(pattern):
                        if file_path.is_file():
                            found_configs[category].append(file_path)
                else:
                    # ç²¾ç¡®åŒ¹é…
                    file_path = self.project_root / pattern
                    if file_path.exists() and file_path.is_file():
                        found_configs[category].append(file_path)
                        
        return found_configs
        
    def analyze_config_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªé…ç½®æ–‡ä»¶
        
        Args:
            file_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æç»“æœ
        """
        analysis = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'file_size': file_path.stat().st_size,
            'last_modified': datetime.fromtimestamp(file_path.stat().st_mtime),
            'file_type': self._detect_file_type(file_path),
            'issues': [],
            'suggestions': [],
            'content_summary': {}
        }
        
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            content = file_path.read_text(encoding='utf-8')
            analysis['line_count'] = len(content.splitlines())
            analysis['char_count'] = len(content)
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œç‰¹å®šåˆ†æ
            if file_path.name == 'pyproject.toml':
                analysis.update(self._analyze_pyproject_toml(file_path, content))
            elif file_path.name == 'requirements.txt':
                analysis.update(self._analyze_requirements_txt(file_path, content))
            elif file_path.name == 'package.json':
                analysis.update(self._analyze_package_json(file_path, content))
            elif file_path.name == '.gitignore':
                analysis.update(self._analyze_gitignore(file_path, content))
            elif file_path.name in ['docker-compose.yml', 'docker-compose.yaml']:
                analysis.update(self._analyze_docker_compose(file_path, content))
                
        except Exception as e:
            analysis['issues'].append(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            
        return analysis
        
    def _detect_file_type(self, file_path: Path) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        name = file_path.name.lower()
        suffix = file_path.suffix.lower()
        
        if suffix in ['.json']:
            return 'json'
        elif suffix in ['.toml']:
            return 'toml'
        elif suffix in ['.yml', '.yaml']:
            return 'yaml'
        elif suffix in ['.ini', '.cfg']:
            return 'ini'
        elif suffix in ['.py']:
            return 'python'
        elif suffix in ['.md']:
            return 'markdown'
        elif name in ['dockerfile', '.dockerignore', '.gitignore', '.gitattributes']:
            return 'text'
        else:
            return 'unknown'
            
    def _analyze_pyproject_toml(self, file_path: Path, content: str) -> Dict[str, Any]:
        """åˆ†æ pyproject.toml æ–‡ä»¶"""
        analysis = {'content_summary': {}, 'issues': [], 'suggestions': []}
        
        try:
            data = toml.loads(content)
            
            # åŸºæœ¬ä¿¡æ¯
            if 'project' in data:
                project = data['project']
                analysis['content_summary']['project_name'] = project.get('name', 'Unknown')
                analysis['content_summary']['version'] = project.get('version', 'Unknown')
                analysis['content_summary']['python_requires'] = project.get('requires-python', 'Unknown')
                
                # ä¾èµ–åˆ†æ
                dependencies = project.get('dependencies', [])
                analysis['content_summary']['dependency_count'] = len(dependencies)
                
                optional_deps = project.get('optional-dependencies', {})
                analysis['content_summary']['optional_dependency_groups'] = len(optional_deps)
                
                # æ£€æŸ¥å¸¸è§é—®é¢˜
                if not project.get('description'):
                    analysis['issues'].append("ç¼ºå°‘é¡¹ç›®æè¿°")
                    
                if not project.get('authors'):
                    analysis['issues'].append("ç¼ºå°‘ä½œè€…ä¿¡æ¯")
                    
                if not project.get('license'):
                    analysis['suggestions'].append("å»ºè®®æ·»åŠ è®¸å¯è¯ä¿¡æ¯")
                    
            # æ„å»ºç³»ç»Ÿåˆ†æ
            if 'build-system' in data:
                build_system = data['build-system']
                analysis['content_summary']['build_backend'] = build_system.get('build-backend', 'Unknown')
                
            # å·¥å…·é…ç½®åˆ†æ
            tool_configs = []
            for key in data.keys():
                if key.startswith('tool.'):
                    tool_configs.append(key[5:])  # ç§»é™¤ 'tool.' å‰ç¼€
                    
            analysis['content_summary']['configured_tools'] = tool_configs
            
            # æ£€æŸ¥é‡å¤ä¾èµ–
            all_deps = set()
            for dep in dependencies:
                dep_name = dep.split('[')[0].split('>=')[0].split('==')[0].split('~=')[0].strip()
                if dep_name in all_deps:
                    analysis['issues'].append(f"é‡å¤ä¾èµ–: {dep_name}")
                all_deps.add(dep_name)
                
        except Exception as e:
            analysis['issues'].append(f"è§£æ TOML å¤±è´¥: {e}")
            
        return analysis
        
    def _analyze_requirements_txt(self, file_path: Path, content: str) -> Dict[str, Any]:
        """åˆ†æ requirements.txt æ–‡ä»¶"""
        analysis = {'content_summary': {}, 'issues': [], 'suggestions': []}
        
        lines = content.splitlines()
        dependencies = []
        comments = 0
        empty_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                empty_lines += 1
            elif line.startswith('#'):
                comments += 1
            elif line.startswith('-'):
                # pip é€‰é¡¹
                if line.startswith('-r'):
                    analysis['suggestions'].append(f"å¼•ç”¨å…¶ä»–æ–‡ä»¶: {line}")
            else:
                dependencies.append(line)
                
        analysis['content_summary']['dependency_count'] = len(dependencies)
        analysis['content_summary']['comment_lines'] = comments
        analysis['content_summary']['empty_lines'] = empty_lines
        
        # æ£€æŸ¥ç‰ˆæœ¬å›ºå®š
        pinned_versions = 0
        range_versions = 0
        no_versions = 0
        
        for dep in dependencies:
            if '==' in dep:
                pinned_versions += 1
            elif any(op in dep for op in ['>=', '<=', '>', '<', '~=', '!=']):
                range_versions += 1
            else:
                no_versions += 1
                
        analysis['content_summary']['pinned_versions'] = pinned_versions
        analysis['content_summary']['range_versions'] = range_versions
        analysis['content_summary']['no_versions'] = no_versions
        
        if no_versions > 0:
            analysis['issues'].append(f"{no_versions} ä¸ªä¾èµ–æ²¡æœ‰æŒ‡å®šç‰ˆæœ¬")
            
        # æ£€æŸ¥é‡å¤ä¾èµ–
        dep_names = set()
        for dep in dependencies:
            name = dep.split('[')[0].split('>=')[0].split('==')[0].split('~=')[0].strip()
            if name in dep_names:
                analysis['issues'].append(f"é‡å¤ä¾èµ–: {name}")
            dep_names.add(name)
            
        return analysis
        
    def _analyze_package_json(self, file_path: Path, content: str) -> Dict[str, Any]:
        """åˆ†æ package.json æ–‡ä»¶"""
        analysis = {'content_summary': {}, 'issues': [], 'suggestions': []}
        
        try:
            data = json.loads(content)
            
            # åŸºæœ¬ä¿¡æ¯
            analysis['content_summary']['name'] = data.get('name', 'Unknown')
            analysis['content_summary']['version'] = data.get('version', 'Unknown')
            analysis['content_summary']['description'] = data.get('description', '')
            
            # ä¾èµ–åˆ†æ
            deps = data.get('dependencies', {})
            dev_deps = data.get('devDependencies', {})
            peer_deps = data.get('peerDependencies', {})
            
            analysis['content_summary']['dependency_count'] = len(deps)
            analysis['content_summary']['dev_dependency_count'] = len(dev_deps)
            analysis['content_summary']['peer_dependency_count'] = len(peer_deps)
            
            # è„šæœ¬åˆ†æ
            scripts = data.get('scripts', {})
            analysis['content_summary']['script_count'] = len(scripts)
            
            # æ£€æŸ¥å¸¸è§é—®é¢˜
            if not data.get('description'):
                analysis['issues'].append("ç¼ºå°‘é¡¹ç›®æè¿°")
                
            if not data.get('author'):
                analysis['issues'].append("ç¼ºå°‘ä½œè€…ä¿¡æ¯")
                
            if not data.get('license'):
                analysis['suggestions'].append("å»ºè®®æ·»åŠ è®¸å¯è¯ä¿¡æ¯")
                
            # æ£€æŸ¥é‡å¤ä¾èµ–
            all_deps = set(deps.keys()) | set(dev_deps.keys()) | set(peer_deps.keys())
            if len(all_deps) < len(deps) + len(dev_deps) + len(peer_deps):
                analysis['issues'].append("å­˜åœ¨é‡å¤ä¾èµ–")
                
        except Exception as e:
            analysis['issues'].append(f"è§£æ JSON å¤±è´¥: {e}")
            
        return analysis
        
    def _analyze_gitignore(self, file_path: Path, content: str) -> Dict[str, Any]:
        """åˆ†æ .gitignore æ–‡ä»¶"""
        analysis = {'content_summary': {}, 'issues': [], 'suggestions': []}
        
        lines = content.splitlines()
        patterns = []
        comments = 0
        empty_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                empty_lines += 1
            elif line.startswith('#'):
                comments += 1
            else:
                patterns.append(line)
                
        analysis['content_summary']['pattern_count'] = len(patterns)
        analysis['content_summary']['comment_lines'] = comments
        analysis['content_summary']['empty_lines'] = empty_lines
        
        # æ£€æŸ¥å¸¸è§æ¨¡å¼
        common_patterns = {
            '__pycache__/': 'Python ç¼“å­˜',
            '*.pyc': 'Python å­—èŠ‚ç ',
            '.env': 'ç¯å¢ƒå˜é‡',
            'node_modules/': 'Node.js æ¨¡å—',
            '.DS_Store': 'macOS ç³»ç»Ÿæ–‡ä»¶',
            '*.log': 'æ—¥å¿—æ–‡ä»¶',
            '.vscode/': 'VS Code é…ç½®',
            '.idea/': 'IntelliJ IDEA é…ç½®'
        }
        
        missing_patterns = []
        for pattern, description in common_patterns.items():
            if not any(p.strip() == pattern for p in patterns):
                missing_patterns.append(f"{pattern} ({description})")
                
        if missing_patterns:
            analysis['suggestions'].append(f"å»ºè®®æ·»åŠ å¸¸è§å¿½ç•¥æ¨¡å¼: {', '.join(missing_patterns)}")
            
        # æ£€æŸ¥é‡å¤æ¨¡å¼
        unique_patterns = set(patterns)
        if len(unique_patterns) < len(patterns):
            analysis['issues'].append("å­˜åœ¨é‡å¤çš„å¿½ç•¥æ¨¡å¼")
            
        return analysis
        
    def _analyze_docker_compose(self, file_path: Path, content: str) -> Dict[str, Any]:
        """åˆ†æ docker-compose.yml æ–‡ä»¶"""
        analysis = {'content_summary': {}, 'issues': [], 'suggestions': []}
        
        try:
            data = yaml.safe_load(content)
            
            # åŸºæœ¬ä¿¡æ¯
            version = data.get('version', 'Unknown')
            analysis['content_summary']['compose_version'] = version
            
            # æœåŠ¡åˆ†æ
            services = data.get('services', {})
            analysis['content_summary']['service_count'] = len(services)
            
            service_info = []
            for service_name, service_config in services.items():
                info = {'name': service_name}
                
                if 'image' in service_config:
                    info['image'] = service_config['image']
                elif 'build' in service_config:
                    info['build'] = True
                    
                if 'ports' in service_config:
                    info['ports'] = len(service_config['ports'])
                    
                if 'volumes' in service_config:
                    info['volumes'] = len(service_config['volumes'])
                    
                service_info.append(info)
                
            analysis['content_summary']['services'] = service_info
            
            # ç½‘ç»œåˆ†æ
            networks = data.get('networks', {})
            analysis['content_summary']['network_count'] = len(networks)
            
            # å·åˆ†æ
            volumes = data.get('volumes', {})
            analysis['content_summary']['volume_count'] = len(volumes)
            
            # æ£€æŸ¥å¸¸è§é—®é¢˜
            if version and version.startswith('2'):
                analysis['suggestions'].append("å»ºè®®å‡çº§åˆ° Compose æ–‡ä»¶æ ¼å¼ç‰ˆæœ¬ 3.x")
                
            for service_name, service_config in services.items():
                if 'restart' not in service_config:
                    analysis['suggestions'].append(f"æœåŠ¡ {service_name} å»ºè®®æ·»åŠ é‡å¯ç­–ç•¥")
                    
                if 'image' in service_config and ':latest' in service_config['image']:
                    analysis['issues'].append(f"æœåŠ¡ {service_name} ä½¿ç”¨ latest æ ‡ç­¾ï¼Œå»ºè®®æŒ‡å®šå…·ä½“ç‰ˆæœ¬")
                    
        except Exception as e:
            analysis['issues'].append(f"è§£æ YAML å¤±è´¥: {e}")
            
        return analysis
        
    def backup_config_file(self, file_path: Path) -> Path:
        """å¤‡ä»½é…ç½®æ–‡ä»¶
        
        Args:
            file_path: è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{file_path.name}.{timestamp}.backup"
        backup_path = self.backup_dir / backup_name
        
        shutil.copy2(file_path, backup_path)
        return backup_path
        
    def optimize_config_file(self, file_path: Path) -> Dict[str, Any]:
        """ä¼˜åŒ–å•ä¸ªé…ç½®æ–‡ä»¶
        
        Args:
            file_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        result = {
            'file_path': str(file_path),
            'optimized': False,
            'backup_path': None,
            'changes': [],
            'errors': []
        }
        
        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = self.backup_config_file(file_path)
            result['backup_path'] = str(backup_path)
            
            # æ ¹æ®æ–‡ä»¶åé€‰æ‹©ä¼˜åŒ–è§„åˆ™
            file_name = file_path.name
            if file_name in self.optimization_rules:
                optimizer = self.optimization_rules[file_name]
                changes = optimizer(file_path)
                
                if changes:
                    result['optimized'] = True
                    result['changes'] = changes
                else:
                    result['changes'] = ['æ— éœ€ä¼˜åŒ–']
                    
            else:
                result['changes'] = ['æš‚æ— ä¼˜åŒ–è§„åˆ™']
                
        except Exception as e:
            result['errors'].append(str(e))
            
        return result
        
    def _optimize_pyproject_toml(self, file_path: Path) -> List[str]:
        """ä¼˜åŒ– pyproject.toml æ–‡ä»¶"""
        changes = []
        
        try:
            content = file_path.read_text(encoding='utf-8')
            data = toml.loads(content)
            
            # æ’åºä¾èµ–
            if 'project' in data and 'dependencies' in data['project']:
                original_deps = data['project']['dependencies']
                sorted_deps = sorted(original_deps, key=lambda x: x.lower())
                
                if original_deps != sorted_deps:
                    data['project']['dependencies'] = sorted_deps
                    changes.append("ä¾èµ–åˆ—è¡¨å·²æ’åº")
                    
            # æ’åºå¯é€‰ä¾èµ–
            if 'project' in data and 'optional-dependencies' in data['project']:
                optional_deps = data['project']['optional-dependencies']
                for group_name, deps in optional_deps.items():
                    sorted_deps = sorted(deps, key=lambda x: x.lower())
                    if deps != sorted_deps:
                        optional_deps[group_name] = sorted_deps
                        changes.append(f"å¯é€‰ä¾èµ–ç»„ {group_name} å·²æ’åº")
                        
            # ä¿å­˜ä¿®æ”¹
            if changes:
                with open(file_path, 'w', encoding='utf-8') as f:
                    toml.dump(data, f)
                    
        except Exception as e:
            changes.append(f"ä¼˜åŒ–å¤±è´¥: {e}")
            
        return changes
        
    def _optimize_requirements_txt(self, file_path: Path) -> List[str]:
        """ä¼˜åŒ– requirements.txt æ–‡ä»¶"""
        changes = []
        
        try:
            lines = file_path.read_text(encoding='utf-8').splitlines()
            
            # åˆ†ç¦»ä¾èµ–å’Œå…¶ä»–å†…å®¹
            dependencies = []
            other_lines = []
            
            for line in lines:
                stripped = line.strip()
                if stripped and not stripped.startswith('#') and not stripped.startswith('-'):
                    dependencies.append(line)
                else:
                    other_lines.append(line)
                    
            # æ’åºä¾èµ–
            original_deps = dependencies.copy()
            dependencies.sort(key=lambda x: x.lower())
            
            if original_deps != dependencies:
                changes.append("ä¾èµ–åˆ—è¡¨å·²æ’åº")
                
            # é‡æ–°ç»„åˆå†…å®¹
            new_content = []
            
            # æ·»åŠ æ³¨é‡Šå’Œé€‰é¡¹
            for line in other_lines:
                if line.strip():
                    new_content.append(line)
                    
            # æ·»åŠ ç©ºè¡Œåˆ†éš”
            if new_content and dependencies:
                new_content.append('')
                
            # æ·»åŠ æ’åºåçš„ä¾èµ–
            new_content.extend(dependencies)
            
            # ä¿å­˜ä¿®æ”¹
            if changes:
                file_path.write_text('\n'.join(new_content) + '\n', encoding='utf-8')
                
        except Exception as e:
            changes.append(f"ä¼˜åŒ–å¤±è´¥: {e}")
            
        return changes
        
    def _optimize_package_json(self, file_path: Path) -> List[str]:
        """ä¼˜åŒ– package.json æ–‡ä»¶"""
        changes = []
        
        try:
            content = file_path.read_text(encoding='utf-8')
            data = json.loads(content)
            
            # æ’åºä¾èµ–
            for dep_type in ['dependencies', 'devDependencies', 'peerDependencies']:
                if dep_type in data:
                    original_deps = data[dep_type]
                    sorted_deps = dict(sorted(original_deps.items()))
                    
                    if original_deps != sorted_deps:
                        data[dep_type] = sorted_deps
                        changes.append(f"{dep_type} å·²æ’åº")
                        
            # æ’åºè„šæœ¬
            if 'scripts' in data:
                original_scripts = data['scripts']
                sorted_scripts = dict(sorted(original_scripts.items()))
                
                if original_scripts != sorted_scripts:
                    data['scripts'] = sorted_scripts
                    changes.append("è„šæœ¬å·²æ’åº")
                    
            # ä¿å­˜ä¿®æ”¹
            if changes:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                    
        except Exception as e:
            changes.append(f"ä¼˜åŒ–å¤±è´¥: {e}")
            
        return changes
        
    def _optimize_gitignore(self, file_path: Path) -> List[str]:
        """ä¼˜åŒ– .gitignore æ–‡ä»¶"""
        changes = []
        
        try:
            lines = file_path.read_text(encoding='utf-8').splitlines()
            
            # åˆ†ç¦»æ¨¡å¼å’Œæ³¨é‡Š
            patterns = []
            comments = []
            
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('#'):
                    comments.append(line)
                elif stripped:
                    patterns.append(line)
                    
            # å»é‡å¹¶æ’åºæ¨¡å¼
            unique_patterns = list(dict.fromkeys(patterns))  # ä¿æŒé¡ºåºçš„å»é‡
            
            if len(unique_patterns) < len(patterns):
                changes.append("ç§»é™¤é‡å¤æ¨¡å¼")
                
            # é‡æ–°ç»„åˆå†…å®¹
            new_content = []
            
            # æ·»åŠ æ³¨é‡Š
            if comments:
                new_content.extend(comments)
                new_content.append('')
                
            # æ·»åŠ æ¨¡å¼
            new_content.extend(unique_patterns)
            
            # ä¿å­˜ä¿®æ”¹
            if changes:
                file_path.write_text('\n'.join(new_content) + '\n', encoding='utf-8')
                
        except Exception as e:
            changes.append(f"ä¼˜åŒ–å¤±è´¥: {e}")
            
        return changes
        
    def _optimize_docker_compose(self, file_path: Path) -> List[str]:
        """ä¼˜åŒ– docker-compose.yml æ–‡ä»¶"""
        changes = []
        
        try:
            content = file_path.read_text(encoding='utf-8')
            data = yaml.safe_load(content)
            
            # æ£€æŸ¥å¹¶æ›´æ–°ç‰ˆæœ¬
            if 'version' in data and data['version'].startswith('2'):
                data['version'] = '3.8'
                changes.append("å‡çº§ Compose æ–‡ä»¶æ ¼å¼ç‰ˆæœ¬åˆ° 3.8")
                
            # ä¸ºæœåŠ¡æ·»åŠ é‡å¯ç­–ç•¥
            if 'services' in data:
                for service_name, service_config in data['services'].items():
                    if 'restart' not in service_config:
                        service_config['restart'] = 'unless-stopped'
                        changes.append(f"ä¸ºæœåŠ¡ {service_name} æ·»åŠ é‡å¯ç­–ç•¥")
                        
            # ä¿å­˜ä¿®æ”¹
            if changes:
                with open(file_path, 'w', encoding='utf-8') as f:
                    yaml.dump(data, f, default_flow_style=False, allow_unicode=True)
                    
        except Exception as e:
            changes.append(f"ä¼˜åŒ–å¤±è´¥: {e}")
            
        return changes
        
    def generate_config_report(self, analyses: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆé…ç½®åˆ†ææŠ¥å‘Š
        
        Args:
            analyses: åˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        report = f"""
# é…ç½®æ–‡ä»¶åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}

## æ¦‚è§ˆ

"""
        
        total_files = len(analyses)
        total_issues = sum(len(a.get('issues', [])) for a in analyses)
        total_suggestions = sum(len(a.get('suggestions', [])) for a in analyses)
        
        report += f"""
- æ€»é…ç½®æ–‡ä»¶æ•°: {total_files}
- å‘ç°é—®é¢˜æ•°: {total_issues}
- ä¼˜åŒ–å»ºè®®æ•°: {total_suggestions}

## è¯¦ç»†åˆ†æ

"""
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        categories = {}
        for analysis in analyses:
            file_path = Path(analysis['file_path'])
            category = 'other'
            
            for cat, patterns in self.config_patterns.items():
                for pattern in patterns.keys():
                    if '*' in pattern:
                        if file_path.match(pattern):
                            category = cat
                            break
                    else:
                        if file_path.name == pattern or str(file_path).endswith(pattern):
                            category = cat
                            break
                if category != 'other':
                    break
                    
            if category not in categories:
                categories[category] = []
            categories[category].append(analysis)
            
        for category, files in categories.items():
            report += f"\n### {category.upper()} é…ç½®\n\n"
            
            for analysis in files:
                file_name = Path(analysis['file_path']).name
                issues = analysis.get('issues', [])
                suggestions = analysis.get('suggestions', [])
                
                status_emoji = "âŒ" if issues else "âœ…"
                report += f"#### {status_emoji} {file_name}\n\n"
                
                # åŸºæœ¬ä¿¡æ¯
                report += f"- **è·¯å¾„**: `{analysis['file_path']}`\n"
                report += f"- **å¤§å°**: {analysis.get('file_size', 0)} å­—èŠ‚\n"
                report += f"- **è¡Œæ•°**: {analysis.get('line_count', 0)}\n"
                report += f"- **æœ€åä¿®æ”¹**: {analysis.get('last_modified', 'Unknown')}\n"
                
                # å†…å®¹æ‘˜è¦
                if 'content_summary' in analysis:
                    summary = analysis['content_summary']
                    if summary:
                        report += "\n**å†…å®¹æ‘˜è¦**:\n"
                        for key, value in summary.items():
                            report += f"- {key}: {value}\n"
                            
                # é—®é¢˜
                if issues:
                    report += "\n**é—®é¢˜**:\n"
                    for issue in issues:
                        report += f"- âŒ {issue}\n"
                        
                # å»ºè®®
                if suggestions:
                    report += "\n**å»ºè®®**:\n"
                    for suggestion in suggestions:
                        report += f"- ğŸ’¡ {suggestion}\n"
                        
                report += "\n"
                
        return report
        
    def run_full_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„é…ç½®åˆ†æ
        
        Returns:
            åˆ†æç»“æœ
        """
        print("ğŸ” æ‰«æé…ç½®æ–‡ä»¶...")
        config_files = self.scan_config_files()
        
        all_files = []
        for category, files in config_files.items():
            all_files.extend(files)
            
        print(f"ğŸ“ å‘ç° {len(all_files)} ä¸ªé…ç½®æ–‡ä»¶")
        
        analyses = []
        for i, file_path in enumerate(all_files, 1):
            print(f"[{i}/{len(all_files)}] åˆ†æ {file_path.name}...")
            analysis = self.analyze_config_file(file_path)
            analyses.append(analysis)
            
        return {
            'config_files': config_files,
            'analyses': analyses,
            'summary': {
                'total_files': len(all_files),
                'total_issues': sum(len(a.get('issues', [])) for a in analyses),
                'total_suggestions': sum(len(a.get('suggestions', [])) for a in analyses)
            }
        }
        
    def run_optimization(self, file_patterns: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œé…ç½®ä¼˜åŒ–
        
        Args:
            file_patterns: è¦ä¼˜åŒ–çš„æ–‡ä»¶æ¨¡å¼åˆ—è¡¨
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        config_files = self.scan_config_files()
        
        # ç¡®å®šè¦ä¼˜åŒ–çš„æ–‡ä»¶
        files_to_optimize = []
        
        if file_patterns:
            # æ ¹æ®æ¨¡å¼ç­›é€‰æ–‡ä»¶
            for pattern in file_patterns:
                for category, files in config_files.items():
                    for file_path in files:
                        if pattern in file_path.name or pattern in str(file_path):
                            files_to_optimize.append(file_path)
        else:
            # ä¼˜åŒ–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
            for category, files in config_files.items():
                for file_path in files:
                    if file_path.name in self.optimization_rules:
                        files_to_optimize.append(file_path)
                        
        # å»é‡
        files_to_optimize = list(set(files_to_optimize))
        
        print(f"ğŸ”§ å‡†å¤‡ä¼˜åŒ– {len(files_to_optimize)} ä¸ªé…ç½®æ–‡ä»¶")
        
        results = []
        for i, file_path in enumerate(files_to_optimize, 1):
            print(f"[{i}/{len(files_to_optimize)}] ä¼˜åŒ– {file_path.name}...")
            result = self.optimize_config_file(file_path)
            results.append(result)
            
        return {
            'optimized_files': len(files_to_optimize),
            'results': results,
            'summary': {
                'successful': len([r for r in results if r['optimized']]),
                'failed': len([r for r in results if r['errors']]),
                'no_changes': len([r for r in results if not r['optimized'] and not r['errors']])
            }
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="é…ç½®æ–‡ä»¶ä¼˜åŒ–å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--project-root',
        default='.',
        help='é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: å½“å‰ç›®å½•ï¼‰'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æé…ç½®æ–‡ä»¶')
    analyze_parser.add_argument('--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    
    # ä¼˜åŒ–å‘½ä»¤
    optimize_parser = subparsers.add_parser('optimize', help='ä¼˜åŒ–é…ç½®æ–‡ä»¶')
    optimize_parser.add_argument('--files', nargs='*', help='è¦ä¼˜åŒ–çš„æ–‡ä»¶æ¨¡å¼')
    optimize_parser.add_argument('--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    optimizer = ConfigOptimizer(args.project_root)
    
    if args.command == 'analyze':
        result = optimizer.run_full_analysis()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = optimizer.generate_config_report(result['analyses'])
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(report)
            
    elif args.command == 'optimize':
        result = optimizer.run_optimization(args.files)
        
        print(f"\nğŸ“Š ä¼˜åŒ–æ‘˜è¦:")
        print(f"- æˆåŠŸä¼˜åŒ–: {result['summary']['successful']} ä¸ªæ–‡ä»¶")
        print(f"- ä¼˜åŒ–å¤±è´¥: {result['summary']['failed']} ä¸ªæ–‡ä»¶")
        print(f"- æ— éœ€ä¼˜åŒ–: {result['summary']['no_changes']} ä¸ªæ–‡ä»¶")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False, default=str)
            print(f"ğŸ“Š ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            
    else:
        print("ğŸ”§ é…ç½®æ–‡ä»¶ä¼˜åŒ–å™¨")
        print("\nä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†å¸®åŠ©ä¿¡æ¯")
        print("\nç¤ºä¾‹:")
        print("  python config_optimizer.py analyze")
        print("  python config_optimizer.py optimize --files pyproject.toml")
        
if __name__ == "__main__":
    main()