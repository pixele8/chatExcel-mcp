from mcp.server.fastmcp import FastMCP
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
import base64
import os
from chardet import detect
import traceback
from io import StringIO
import sys
import time
from textwrap import wrap
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from column_checker import ColumnChecker
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows
from excel_helper import _suggest_excel_read_parameters, detect_excel_structure
# å¯¼å…¥Excelæ™ºèƒ½å·¥å…·
from excel_smart_tools import suggest_excel_read_parameters, detect_excel_file_structure, create_excel_read_template
from enhanced_excel_helper import smart_read_excel, detect_file_encoding, validate_excel_data_integrity
from comprehensive_data_verification import ComprehensiveDataVerifier
from data_verification import verify_data_processing_result, DataVerificationEngine
from excel_enhanced_tools import ExcelEnhancedProcessor, get_excel_processor
# å¯¼å…¥ Excel å…¬å¼å¤„ç†å·¥å…·
from formulas_tools import (
    parse_excel_formula,
    compile_excel_workbook,
    execute_excel_formula,
    analyze_excel_dependencies,
    validate_excel_formula
)
# å¯¼å…¥ Excel æ•°æ®è´¨é‡å·¥å…·
from excel_data_quality_tools import (
    ExcelDataQualityController,
    ExcelCellContentExtractor,
    ExcelCharacterConverter,
    ExcelMultiConditionExtractor,
    ExcelMultiTableMerger,
    ExcelDataCleaner,
    ExcelBatchProcessor
)

# ç»Ÿä¸€å¸¸é‡å®šä¹‰ - å®Œå…¨è§£é™¤æ‰€æœ‰å®‰å…¨é™åˆ¶
MAX_FILE_SIZE = 999999999999  # æ— é™åˆ¶æ–‡ä»¶å¤§å°
BLACKLIST = []  # å®Œå…¨æ¸…ç©ºé»‘åå•ï¼Œå…è®¸æ‰€æœ‰æ“ä½œåŒ…æ‹¬subprocessç­‰
TEMPLATE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "templates")
CHARTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "charts")

# åˆå§‹åŒ–æ•°æ®è´¨é‡å·¥å…·å®ä¾‹
data_quality_controller = ExcelDataQualityController()
cell_content_extractor = ExcelCellContentExtractor()
character_converter = ExcelCharacterConverter()
multi_condition_extractor = ExcelMultiConditionExtractor()
multi_table_merger = ExcelMultiTableMerger()
data_cleaner = ExcelDataCleaner()
batch_processor = ExcelBatchProcessor()

# é”™è¯¯ç±»å‹å¸¸é‡
ERROR_TYPES = {
    "FILE_NOT_FOUND": "æŒ‡å®šçš„æ–‡ä»¶æœªæ‰¾åˆ°",
    "FILE_TOO_LARGE": "æ–‡ä»¶å¤§å°è¶…è¿‡æœ€å¤§é™åˆ¶",
    "TEMPLATE_ERROR": "æ¨¡æ¿å¤„ç†å¤±è´¥",
    "SECURITY_VIOLATION": "æ“ä½œå› å®‰å…¨åŸå› è¢«é˜»æ­¢",
    "VALIDATION_ERROR": "è¾“å…¥éªŒè¯å¤±è´¥",
    "PROCESSING_ERROR": "æ•°æ®å¤„ç†å¤±è´¥"
}

mcp = FastMCP("chatExcel")

# å·¥å…·å‡½æ•°
def create_error_response(error_type: str, message: str, details: dict = None, solutions: list = None) -> dict:
    """åˆ›å»ºç»Ÿä¸€æ ¼å¼çš„é”™è¯¯å“åº”
    
    Args:
        error_type: é”™è¯¯ç±»å‹
        message: é”™è¯¯æ¶ˆæ¯
        details: é”™è¯¯è¯¦æƒ…
        solutions: è§£å†³æ–¹æ¡ˆå»ºè®®
        
    Returns:
        dict: æ ‡å‡†åŒ–é”™è¯¯å“åº”
    """
    response = {
        "status": "ERROR",
        "error_type": error_type,
        "message": message,
        "timestamp": time.time()
    }
    
    if details:
        response["details"] = details
    if solutions:
        response["solutions"] = solutions
        
    return response

def create_success_response(data: dict, message: str = "æ“ä½œæˆåŠŸå®Œæˆ") -> dict:
    """åˆ›å»ºç»Ÿä¸€æ ¼å¼çš„æˆåŠŸå“åº”
    
    Args:
        data: å“åº”æ•°æ®
        message: æˆåŠŸæ¶ˆæ¯
        
    Returns:
        dict: æ ‡å‡†åŒ–æˆåŠŸå“åº”
    """
    return {
        "status": "SUCCESS",
        "message": message,
        "data": data,
        "timestamp": time.time()
    }

def get_template_path(template_name: str) -> str:
    """è·å–æ¨¡æ¿æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    
    Args:
        template_name: æ¨¡æ¿æ–‡ä»¶å
        
    Returns:
        str: æ¨¡æ¿æ–‡ä»¶ç»å¯¹è·¯å¾„
        
    Raises:
        FileNotFoundError: æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨
    """
    template_path = os.path.join(TEMPLATE_DIR, template_name)
    
    if not os.path.exists(template_path):
        raise FileNotFoundError(f"æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
    
    return template_path

def validate_file_access(file_path: str) -> dict:
    """éªŒè¯æ–‡ä»¶è®¿é—®æƒé™å’Œå¤§å°ï¼ˆå®½æ¾æ¨¡å¼ï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: éªŒè¯ç»“æœï¼ŒåŒ…å«statuså’Œç›¸å…³ä¿¡æ¯
    """
    # å®½æ¾æ¨¡å¼ï¼šå³ä½¿æ–‡ä»¶ä¸å­˜åœ¨ä¹Ÿå…è®¸æ‰§è¡Œï¼ˆå¯èƒ½æ˜¯è¦åˆ›å»ºæ–°æ–‡ä»¶ï¼‰
    if not os.path.exists(file_path):
        logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½†å…è®¸æ‰§è¡Œ: {file_path}")
        return {"status": "SUCCESS", "file_size": 0, "note": "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¼šåˆ›å»ºæ–°æ–‡ä»¶"}

    file_size = os.path.getsize(file_path)
    # ç§»é™¤æ–‡ä»¶å¤§å°é™åˆ¶
    logger.debug(f"æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f}MB")
    
    return {"status": "SUCCESS", "file_size": file_size}

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chatExcel.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@mcp.tool()
def read_metadata(file_path: str) -> dict:
    """è¯»å–CSVæ–‡ä»¶å…ƒæ•°æ®å¹¶è¿”å›MCPå…¼å®¹æ ¼å¼
    
    Args:
        file_path: CSVæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        
    Returns:
        dict: åŒ…å«åˆ—ä¿¡æ¯ã€æ–‡ä»¶ä¿¡æ¯å’ŒçŠ¶æ€çš„ç»“æ„åŒ–å…ƒæ•°æ®
    """
    logger.info(f"å¼€å§‹è¯»å–æ–‡ä»¶å…ƒæ•°æ®: {file_path}")
    
    try:
        # éªŒè¯æ–‡ä»¶è®¿é—®
        validation_result = validate_file_access(file_path)
        if validation_result["status"] != "SUCCESS":
            return validation_result
        
        # Detect encoding and delimiter
        with open(file_path, 'rb') as f:
            rawdata = f.read(50000)
            enc = detect(rawdata)['encoding'] or 'utf-8'

        with open(file_path, 'r', encoding=enc) as f:
            first_line = f.readline()
            delimiter = ',' if ',' in first_line else '\t' if '\t' in first_line else ';'

        # è·å–æ–‡ä»¶å¤§å°
        file_size = validation_result["file_size"]
        
        # Get actual row count efficiently
        try:
            # Count total rows without loading data
            total_rows = sum(1 for _ in open(file_path, 'r', encoding=enc)) - 1  # -1 for header
        except Exception:
            # Fallback: use pandas to count rows
            # Read only first column to count rows efficiently
            temp_df = pd.read_csv(file_path, encoding=enc, delimiter=delimiter, usecols=[0])
            total_rows = len(temp_df)
        
        # Check file size limit
        if file_size > MAX_FILE_SIZE:
            return {
                "status": "ERROR",
                "error": "FILE_TOO_LARGE",
                "max_size": f"{MAX_FILE_SIZE / 1024 / 1024}MB",
                "actual_size": f"{file_size / 1024 / 1024:.1f}MB"
            }
        
        # Read sample data for metadata analysis
        sample_size = min(100, total_rows)
        df = pd.read_csv(file_path, encoding=enc, delimiter=delimiter, nrows=sample_size)

        # Calculate additional metadata
        columns_metadata = []
        for col in df.columns:
            col_meta = {
                "name": col,
                "type": str(df[col].dtype),
                "sample": df[col].dropna().iloc[:2].tolist(),
                "stats": {
                    "null_count": df[col].isnull().sum(),
                    "unique_count": df[col].nunique(),
                    "is_numeric": pd.api.types.is_numeric_dtype(df[col])
                },
                "warnings": [],
                "suggested_operations": []
            }
            
            # Add numeric stats if applicable
            if pd.api.types.is_numeric_dtype(df[col]):
                col_meta["stats"].update({
                    "min": df[col].min(),
                    "max": df[col].max(),
                    "mean": df[col].mean(),
                    "std": df[col].std()
                })
                col_meta["suggested_operations"].extend([
                    "normalize", "scale", "log_transform"
                ])
            
            # Add categorical stats if applicable
            if pd.api.types.is_string_dtype(df[col]):
                col_meta["suggested_operations"].extend([
                    "one_hot_encode", "label_encode", "text_processing"
                ])
            
            # Add datetime detection
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                col_meta["suggested_operations"].extend([
                    "extract_year", "extract_month", "time_delta"
                ])
            
            # Add warnings
            if df[col].isnull().sum() > 0:
                col_meta["warnings"].append(f"{df[col].isnull().sum()} null values found")
            if df[col].nunique() == 1:
                col_meta["warnings"].append("Column contains only one unique value")
            if pd.api.types.is_numeric_dtype(df[col]) and df[col].abs().max() > 1e6:
                col_meta["warnings"].append("Large numeric values detected - consider scaling")
            
            columns_metadata.append(col_meta)

        from pandas.api.types import infer_dtype
        
        # Format concise response
        summary = {
            "status": "SUCCESS",
            "file_info": {
                "size": f"{file_size / 1024:.1f}KB",
                "encoding": enc,
                "delimiter": delimiter
            },
            "dataset": {
                "total_rows": total_rows,
                "sample_rows": len(df),
                "columns": len(df.columns),
                "column_types": {
                    col: infer_dtype(df[col])
                    for col in df.columns
                }
            },
            "warnings": {
                "message": "Data quality issues detected" if (
                    df.isnull().any().any() or 
                    df.duplicated().any() or
                    (df.nunique() == 1).any()
                ) else "No significant data quality issues found",
                **({
                    "null_columns": {
                        "count": sum(df.isnull().any()),
                        "columns": [col for col in df.columns if df[col].isnull().any()]
                    }
                } if sum(df.isnull().any()) > 0 else {}),
                **({
                    "total_nulls": df.isnull().sum().sum()
                } if df.isnull().sum().sum() > 0 else {}),
                **({
                    "duplicate_rows": {
                        "count": df.duplicated().sum(),
                        "rows": df[df.duplicated()].index.tolist()
                    }
                } if df.duplicated().sum() > 0 else {}),
                **({
                    "single_value_columns": {
                        "count": sum(df.nunique() == 1),
                        "columns": [col for col in df.columns if df[col].nunique() == 1]
                    }
                } if sum(df.nunique() == 1) > 0 else {})
            }
        }
            
        return summary

    except Exception as e:
        return {
            "status": "ERROR",
            "error_type": type(e).__name__,
            "message": str(e),
            "solution": [
                "Check if the file is being used by another program",
                "Try saving the file as UTF-8 encoded CSV",
                "Contact the administrator to check MCP file access permissions"
            ],
            "traceback": traceback.format_exc()
        }




@mcp.tool()
def verify_data_integrity(original_file: str, processed_data: Optional[str] = None, 
                         comparison_file: Optional[str] = None, verification_type: str = "basic") -> dict:
    """æ•°æ®å®Œæ•´æ€§éªŒè¯å’Œæ¯”å¯¹æ ¸å‡†å·¥å…·ã€‚
    
    Args:
        original_file: åŸå§‹Excelæ–‡ä»¶è·¯å¾„
        processed_data: å¤„ç†åçš„æ•°æ®ï¼ˆJSONå­—ç¬¦ä¸²æ ¼å¼ï¼‰æˆ–æ–‡ä»¶è·¯å¾„
        comparison_file: ç”¨äºæ¯”è¾ƒçš„å¦ä¸€ä¸ªExcelæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        verification_type: éªŒè¯ç±»å‹ ("basic", "detailed", "statistical")
    
    Returns:
        dict: éªŒè¯ç»“æœæŠ¥å‘Š
    """
    try:
        from data_verification import DataVerificationEngine, verify_data_processing_result
        
        # éªŒè¯åŸå§‹æ–‡ä»¶è®¿é—®
        validation_result = validate_file_access(original_file)
        if validation_result["status"] != "SUCCESS":
            return {
                "success": False,
                "error": validation_result["message"],
                "suggestion": "è¯·ç¡®ä¿åŸå§‹æ–‡ä»¶è·¯å¾„æ­£ç¡®ä¸”æ–‡ä»¶å­˜åœ¨ã€‚"
            }
        
        # åˆ›å»ºéªŒè¯å¼•æ“
        verifier = DataVerificationEngine()
        
        if comparison_file:
            # æ–‡ä»¶å¯¹æ¯”æ¨¡å¼
            validation_result2 = validate_file_access(comparison_file)
            if validation_result2["status"] != "SUCCESS":
                return {
                    "success": False,
                    "error": f"æ¯”è¾ƒæ–‡ä»¶è®¿é—®å¤±è´¥: {validation_result2['message']}",
                    "suggestion": "è¯·ç¡®ä¿æ¯”è¾ƒæ–‡ä»¶è·¯å¾„æ­£ç¡®ä¸”æ–‡ä»¶å­˜åœ¨ã€‚"
                }
            
            # è¯»å–ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œæ¯”è¾ƒ
            df1 = pd.read_excel(original_file)
            df2 = pd.read_excel(comparison_file)
            
            comparison_result = verifier.compare_dataframes(
                df1, df2, 
                name1=os.path.basename(original_file),
                name2=os.path.basename(comparison_file)
            )
            
            # æ·»åŠ å¢å¼ºçš„æ•°æ®è´¨é‡æ£€æŸ¥
            quality_result1 = data_quality_controller.comprehensive_quality_check(
                original_file, "comprehensive"
            )
            quality_result2 = data_quality_controller.comprehensive_quality_check(
                comparison_file, "comprehensive"
            )
            
            comparison_result["enhanced_quality_analysis"] = {
                "original_file_quality": quality_result1,
                "comparison_file_quality": quality_result2,
                "quality_comparison": {
                    "score_difference": quality_result1.get("overall_score", 0) - quality_result2.get("overall_score", 0),
                    "better_quality_file": "original" if quality_result1.get("overall_score", 0) > quality_result2.get("overall_score", 0) else "comparison"
                }
            }
            
            return {
                "success": True,
                "verification_type": "file_comparison",
                "comparison_result": comparison_result,
                "summary": {
                    "files_compared": [original_file, comparison_file],
                    "match_score": comparison_result.get("match_score", 0),
                    "has_differences": comparison_result.get("has_differences", True)
                }
            }
        
        elif processed_data:
            # æ•°æ®å¤„ç†ç»“æœéªŒè¯æ¨¡å¼
            try:
                # å°è¯•è§£æå¤„ç†åçš„æ•°æ®
                if isinstance(processed_data, str):
                    if processed_data.endswith(('.xlsx', '.xls', '.csv')):
                        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
                        if processed_data.endswith('.csv'):
                            processed_df = pd.read_csv(processed_data)
                        else:
                            processed_df = pd.read_excel(processed_data)
                    else:
                        # å¦‚æœæ˜¯JSONå­—ç¬¦ä¸²
                        import json
                        data_dict = json.loads(processed_data)
                        processed_df = pd.DataFrame(data_dict)
                else:
                    return {
                        "success": False,
                        "error": "å¤„ç†åæ•°æ®æ ¼å¼ä¸æ”¯æŒ",
                        "suggestion": "è¯·æä¾›JSONå­—ç¬¦ä¸²ã€CSVæˆ–Excelæ–‡ä»¶è·¯å¾„ã€‚"
                    }
                
                # æ‰§è¡ŒéªŒè¯
                verification_result = verify_data_processing_result(
                    original_file, processed_df, verification_type
                )
                
                return {
                    "success": True,
                    "verification_type": "processing_verification",
                    "verification_result": verification_result
                }
                
            except Exception as parse_error:
                return {
                    "success": False,
                    "error": f"æ•°æ®è§£æå¤±è´¥: {str(parse_error)}",
                    "suggestion": "è¯·æ£€æŸ¥å¤„ç†åæ•°æ®çš„æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚"
                }
        
        else:
            # åŸºç¡€å®Œæ•´æ€§æ£€æŸ¥æ¨¡å¼
            df = pd.read_excel(original_file)
            
            # æ‰§è¡ŒåŸºç¡€å®Œæ•´æ€§æ£€æŸ¥
            integrity_report = {
                "file_info": {
                    "path": original_file,
                    "shape": df.shape,
                    "columns": list(df.columns),
                    "dtypes": df.dtypes.to_dict()
                },
                "data_quality": {
                    "total_rows": len(df),
                    "total_columns": len(df.columns),
                    "null_counts": df.isnull().sum().to_dict(),
                    "null_percentage": (df.isnull().sum() / len(df) * 100).to_dict(),
                    "duplicate_rows": df.duplicated().sum(),
                    "memory_usage": df.memory_usage(deep=True).to_dict()
                },
                "column_analysis": {}
            }
            
            # è¯¦ç»†åˆ—åˆ†æ
            for col in df.columns:
                col_info = {
                    "dtype": str(df[col].dtype),
                    "null_count": df[col].isnull().sum(),
                    "unique_count": df[col].nunique(),
                    "sample_values": df[col].dropna().head(3).tolist()
                }
                
                if df[col].dtype in ['int64', 'float64']:
                    col_info.update({
                        "min": df[col].min(),
                        "max": df[col].max(),
                        "mean": df[col].mean(),
                        "std": df[col].std()
                    })
                
                integrity_report["column_analysis"][col] = col_info
            
            return {
                "success": True,
                "verification_type": "integrity_check",
                "integrity_report": integrity_report
            }
    
    except Exception as e:
        return {
            "success": False,
            "error": {
                "type": type(e).__name__,
                "message": str(e),
                "traceback": traceback.format_exc()
            },
            "suggestion": "è¯·æ£€æŸ¥è¾“å…¥å‚æ•°å’Œæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚"
        }

@mcp.tool()
def read_excel_metadata(file_path: str) -> dict:
    """å¢å¼ºç‰ˆExcelæ–‡ä»¶å…ƒæ•°æ®è¯»å–ï¼Œæ”¯æŒæ™ºèƒ½ç¼–ç æ£€æµ‹å’Œå®Œæ•´æ€§éªŒè¯ã€‚
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åŒ…å«æ–‡ä»¶å…ƒæ•°æ®ã€ç¼–ç ä¿¡æ¯ã€å®Œæ•´æ€§éªŒè¯ç»“æœçš„å­—å…¸
    """
    try:
        # éªŒè¯æ–‡ä»¶è®¿é—®
        validation_result = validate_file_access(file_path)
        if validation_result["status"] != "SUCCESS":
            return validation_result

        # æ™ºèƒ½ç¼–ç æ£€æµ‹
        encoding_info = detect_file_encoding(file_path)
        
        # ä½¿ç”¨æ™ºèƒ½è¯»å–åŠŸèƒ½
        read_result = smart_read_excel(file_path, sample_size=100)
        if not read_result['success']:
            return {
                "status": "ERROR",
                "error": "SMART_READ_FAILED",
                "message": read_result.get('error', 'æ™ºèƒ½è¯»å–å¤±è´¥'),
                "encoding_info": encoding_info
            }
        
        df = read_result['dataframe']  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é”®å
        read_params = read_result.get('info', {}).get('read_params', {})  # ä¿®å¤ï¼šè·å–æ­£ç¡®çš„å‚æ•°ä¿¡æ¯
        
        # æ•°æ®å®Œæ•´æ€§éªŒè¯
        integrity_result = validate_excel_data_integrity(file_path, df)
        
        # å¢å¼ºçš„æ•°æ®è´¨é‡æ£€æŸ¥
        quality_result = data_quality_controller.comprehensive_quality_check(
            file_path, "comprehensive"
        )
        
        # é«˜çº§å•å…ƒæ ¼å†…å®¹åˆ†æ
        cell_analysis = cell_content_extractor.extract_cell_content_advanced(
            file_path, extract_type="all"
        )
        
        # è·å–æ‰€æœ‰å·¥ä½œè¡¨ä¿¡æ¯
        workbook = openpyxl.load_workbook(file_path, read_only=True)
        sheet_names = workbook.sheetnames
        sheets_info = {}
        
        for sheet_name in sheet_names:
            sheet_obj = workbook[sheet_name]
            sheets_info[sheet_name] = {
                'max_row': sheet_obj.max_row,
                'max_column': sheet_obj.max_column,
                'has_data': sheet_obj.max_row > 1
            }
        workbook.close()
        
        # æ™ºèƒ½å‚æ•°æ¨èï¼ˆä½¿ç”¨å¢å¼ºç‰ˆï¼‰
        suggested_params = _suggest_excel_read_parameters(file_path, read_params.get('sheet_name'))

        columns_metadata = []
        for col in df.columns:
            col_meta = {
                "name": col,
                "type": str(df[col].dtype),
                "sample": df[col].dropna().iloc[:2].tolist(),
                "stats": {
                    "null_count": df[col].isnull().sum(),
                    "unique_count": df[col].nunique(),
                    "is_numeric": pd.api.types.is_numeric_dtype(df[col])
                },
                "warnings": [],
                "suggested_operations": []
            }
            if pd.api.types.is_numeric_dtype(df[col]):
                col_meta["stats"].update({
                    "min": df[col].min(),
                    "max": df[col].max(),
                    "mean": df[col].mean(),
                    "std": df[col].std()
                })
                col_meta["suggested_operations"].extend([
                    "normalize", "scale", "log_transform"
                ])
            if pd.api.types.is_string_dtype(df[col]):
                col_meta["suggested_operations"].extend([
                    "one_hot_encode", "label_encode", "text_processing"
                ])
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                col_meta["suggested_operations"].extend([
                    "extract_year", "extract_month", "time_delta"
                ])
            if df[col].isnull().sum() > 0:
                col_meta["warnings"].append(f"{df[col].isnull().sum()} null values found")
            if df[col].nunique() == 1:
                col_meta["warnings"].append("Column contains only one unique value")
            if pd.api.types.is_numeric_dtype(df[col]) and df[col].abs().max() > 1e6:
                col_meta["warnings"].append("Large numeric values detected - consider scaling")
            columns_metadata.append(col_meta)

        summary = {
            "status": "SUCCESS",
            "file_info": {
                "size": f"{validation_result['file_size'] / 1024:.1f}KB",
                "sheets": sheet_names,
                "sheets_info": sheets_info,
                "encoding": encoding_info
            },
            "dataset": {
                "total_rows": read_result.get('total_rows', len(df)),
                "sample_rows": len(df),
                "columns": len(df.columns),
                "column_types": {
                    col: str(df[col].dtype)
                    for col in df.columns
                }
            },
            "read_params": read_params,
            "suggested_params": suggested_params,
            "columns_metadata": columns_metadata,
            "integrity_check": integrity_result,
            "enhanced_quality_analysis": quality_result,
            "cell_content_analysis": cell_analysis,
            "warnings": {
                "message": "Data quality issues detected" if (
                    df.isnull().any().any() or 
                    df.duplicated().any() or
                    (df.nunique() == 1).any()
                ) else "No significant data quality issues found",
                **({
                    "null_columns": {
                        "count": sum(df.isnull().any()),
                        "columns": [col for col in df.columns if df[col].isnull().any()]
                    }
                } if sum(df.isnull().any()) > 0 else {}),
                **({
                    "total_nulls": df.isnull().sum().sum()
                } if df.isnull().sum().sum() > 0 else {}),
                **({
                    "duplicate_rows": {
                        "count": df.duplicated().sum(),
                        "rows": df[df.duplicated()].index.tolist()
                    }
                } if df.duplicated().sum() > 0 else {}),
                **({
                    "single_value_columns": {
                        "count": sum(df.nunique() == 1),
                        "columns": [col for col in df.columns if df[col].nunique() == 1]
                    }
                } if sum(df.nunique() == 1) > 0 else {})
            }
        }
        return summary

    except Exception as e:
        return {
            "status": "ERROR",
            "error_type": type(e).__name__,
            "message": str(e),
            "solution": [
                "Ensure the file is a valid Excel file (.xlsx)",
                "Check if the file is being used by another program",
                "Contact the administrator to check MCP file access permissions"
            ],
            "traceback": traceback.format_exc()
        }



def smart_column_matcher(target_column: str, available_columns: list) -> dict:
    """æ™ºèƒ½åˆ—ååŒ¹é…å·¥å…·
    
    Args:
        target_column: ç›®æ ‡åˆ—å
        available_columns: å¯ç”¨çš„åˆ—ååˆ—è¡¨
        
    Returns:
        dict: åŒ¹é…ç»“æœå’Œå»ºè®®
    """
    import difflib
    import re
    
    result = {
        'exact_match': None,
        'close_matches': [],
        'suggestions': [],
        'normalized_matches': []
    }
    
    # 1. ç²¾ç¡®åŒ¹é…
    if target_column in available_columns:
        result['exact_match'] = target_column
        return result
    
    # 2. å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
    target_lower = target_column.lower()
    for col in available_columns:
        if col.lower() == target_lower:
            result['exact_match'] = col
            result['suggestions'].append(f"æ‰¾åˆ°å¤§å°å†™ä¸åŒçš„åŒ¹é…: '{col}'")
            return result
    
    # 3. å»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ååŒ¹é…
    target_normalized = re.sub(r'[\s_-]', '', target_column.lower())
    for col in available_columns:
        col_normalized = re.sub(r'[\s_-]', '', col.lower())
        if col_normalized == target_normalized:
            result['normalized_matches'].append(col)
    
    # 4. æ¨¡ç³ŠåŒ¹é…
    close_matches = difflib.get_close_matches(
        target_column, available_columns, n=5, cutoff=0.6
    )
    result['close_matches'] = close_matches
    
    # 5. ä¸­æ–‡åˆ—åå˜ä½“åŒ¹é…
    chinese_variants = {
        'æ¶ˆè€—æ—¥æœŸ': ['æ¶ˆè´¹æ—¥æœŸ', 'ä½¿ç”¨æ—¥æœŸ', 'æ”¯å‡ºæ—¥æœŸ', 'èŠ±è´¹æ—¥æœŸ', 'æ¶ˆè€—æ—¶é—´', 'æ¶ˆè´¹æ—¶é—´'],
        'æ¶ˆè´¹æ—¥æœŸ': ['æ¶ˆè€—æ—¥æœŸ', 'ä½¿ç”¨æ—¥æœŸ', 'æ”¯å‡ºæ—¥æœŸ', 'èŠ±è´¹æ—¥æœŸ', 'æ¶ˆè´¹æ—¶é—´', 'æ¶ˆè€—æ—¶é—´'],
        'æ—¥æœŸ': ['æ—¶é—´', 'Date', 'date', 'åˆ›å»ºæ—¥æœŸ', 'æ›´æ–°æ—¥æœŸ', 'è®°å½•æ—¥æœŸ'],
        'é‡‘é¢': ['æ•°é‡', 'ä»·æ ¼', 'è´¹ç”¨', 'æˆæœ¬', 'Amount', 'amount', 'æ€»é¢'],
        'åç§°': ['å§“å', 'å“å', 'é¡¹ç›®', 'Name', 'name', 'æ ‡é¢˜'],
        'ç±»å‹': ['åˆ†ç±»', 'ç§ç±»', 'Type', 'type', 'ç±»åˆ«']
    }
    
    if target_column in chinese_variants:
        for variant in chinese_variants[target_column]:
            if variant in available_columns:
                result['suggestions'].append(f"å‘ç°ç›¸ä¼¼åˆ—å: '{variant}'")
    
    return result


@mcp.tool()
def run_excel_code(
    file_path: str,
    code: str, 
    sheet_name: str = None, 
    skiprows: int = None, 
    header: int = None, 
    usecols: str = None, 
    encoding: str = None,
    auto_detect: bool = True,
    allow_file_write: bool = False
) -> dict:
    """å¢å¼ºç‰ˆExcelä»£ç æ‰§è¡Œå·¥å…·ï¼Œå…·å¤‡å¼ºåŒ–çš„pandaså¯¼å…¥å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚
    
    Args:
        code: è¦æ‰§è¡Œçš„æ•°æ®å¤„ç†ä»£ç å­—ç¬¦ä¸²
        file_path: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å¯é€‰ï¼Œå·¥ä½œè¡¨åç§°
        skiprows: å¯é€‰ï¼Œè·³è¿‡çš„è¡Œæ•°
        header: å¯é€‰ï¼Œç”¨ä½œåˆ—åçš„è¡Œå·ã€‚å¯ä»¥æ˜¯æ•´æ•°ã€æ•´æ•°åˆ—è¡¨æˆ–None
        usecols: å¯é€‰ï¼Œè¦è§£æçš„åˆ—ã€‚å¯ä»¥æ˜¯åˆ—ååˆ—è¡¨ã€åˆ—ç´¢å¼•åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
        encoding: æŒ‡å®šç¼–ç ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹æ—¶å¿½ç•¥ï¼‰
        auto_detect: æ˜¯å¦å¯ç”¨æ™ºèƒ½æ£€æµ‹å’Œå‚æ•°ä¼˜åŒ–
        allow_file_write: æ˜¯å¦å…è®¸åœ¨ä»£ç ä¸­å†™å…¥æ–‡ä»¶
        
    Returns:
        dict: æ‰§è¡Œç»“æœæˆ–é”™è¯¯ä¿¡æ¯
    """
    
    # å®Œå…¨ç§»é™¤å®‰å…¨æ£€æŸ¥ - å…è®¸æ‰€æœ‰æ“ä½œ
    logger.debug("å®‰å…¨æ£€æŸ¥å·²å®Œå…¨ç¦ç”¨ï¼Œå…è®¸æ‰€æœ‰ä»£ç æ‰§è¡Œ")
    # ä¸å†è¿›è¡Œä»»ä½•å®‰å…¨æ£€æŸ¥ï¼Œç›´æ¥æ‰§è¡Œä»£ç 

    # éªŒè¯æ–‡ä»¶è®¿é—®
    validation_result = validate_file_access(file_path)
    if validation_result["status"] != "SUCCESS":
        return {
            "error": {
                "type": "FILE_ACCESS_ERROR",
                "message": validation_result["message"],
                "solution": "è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ä¸”æ–‡ä»¶å­˜åœ¨ã€‚"
            }
        }

    # å¢å¼ºçš„æ¨¡å—å¯¼å…¥æœºåˆ¶
    def safe_import_pandas():
        """å®‰å…¨å¯¼å…¥ pandas æ¨¡å—"""
        try:
            import pandas as pd_local
            return pd_local, None
        except ImportError as e:
            error_msg = f"pandas å¯¼å…¥å¤±è´¥: {str(e)}"
            # å°è¯•é‡æ–°å¯¼å…¥
            try:
                import importlib
                importlib.invalidate_caches()
                import pandas as pd_local
                return pd_local, None
            except Exception as e2:
                return None, f"{error_msg}. é‡è¯•å¤±è´¥: {str(e2)}"
    
    def safe_import_numpy():
        """å®‰å…¨å¯¼å…¥ numpy æ¨¡å—"""
        try:
            import numpy as np_local
            return np_local, None
        except ImportError as e:
            error_msg = f"numpy å¯¼å…¥å¤±è´¥: {str(e)}"
            try:
                import importlib
                importlib.invalidate_caches()
                import numpy as np_local
                return np_local, None
            except Exception as e2:
                return None, f"{error_msg}. é‡è¯•å¤±è´¥: {str(e2)}"
    
    # å¯¼å…¥å…³é”®æ¨¡å—
    pd_module, pd_error = safe_import_pandas()
    np_module, np_error = safe_import_numpy()
    
    if pd_module is None:
        return {
            "error": {
                "type": "IMPORT_ERROR",
                "message": "Failed to import pandas",
                "details": pd_error,
                "solution": "è¯·ç¡®ä¿ pandas å·²æ­£ç¡®å®‰è£…: pip install pandas"
            }
        }

    # ä½¿ç”¨æ™ºèƒ½è¯»å–åŠŸèƒ½
    if auto_detect:
        # æ™ºèƒ½ç¼–ç æ£€æµ‹
        encoding_info = detect_file_encoding(file_path)
        
        # æ•°æ®è´¨é‡é¢„æ£€æŸ¥
        quality_precheck = data_quality_controller.comprehensive_quality_check(
            file_path, "basic"
        )
        
        # æ„å»ºè¯»å–å‚æ•°
        read_kwargs = {}
        if sheet_name is not None:
            read_kwargs['sheet_name'] = sheet_name
        if skiprows is not None:
            read_kwargs['skiprows'] = skiprows
        if header is not None:
            read_kwargs['header'] = header
        if encoding is not None:
            read_kwargs['encoding'] = encoding
        elif encoding_info.get('encoding'):
            read_kwargs['encoding'] = encoding_info['encoding']
        if usecols is not None:
            read_kwargs['usecols'] = usecols
        
        # ä½¿ç”¨æ™ºèƒ½è¯»å–ï¼Œç¦ç”¨è‡ªåŠ¨æ£€æµ‹å¹¶æ˜ç¡®æŒ‡å®šheader=0
        # æ”¹è¿›çš„æ™ºèƒ½Excelè¯»å–é€»è¾‘
        try:
            from excel_helper import _suggest_excel_read_parameters
            
            # 1. è·å–å‚æ•°å»ºè®®
            suggestions = _suggest_excel_read_parameters(file_path)
            recommended_params = suggestions.get('recommended_params', {})
            
            # 2. æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°å¤šçº§åˆ—å¤´
            is_multi_level = suggestions.get('analysis', {}).get('multi_level_header_detected', False)
            
            # 3. æ™ºèƒ½å‚æ•°é€‰æ‹© - ç”¨æˆ·å‚æ•°ä¼˜å…ˆ
            if header is not None:
                # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†headerå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è®¾ç½®
                final_params = read_kwargs.copy()
                # ä¸è¦†ç›–ç”¨æˆ·çš„headerè®¾ç½®
            elif is_multi_level:
                # å¤šçº§åˆ—å¤´ä¸”ç”¨æˆ·æœªæŒ‡å®šheaderï¼šä½¿ç”¨å»ºè®®å‚æ•°
                final_params = recommended_params.copy()
                final_params.update(read_kwargs)
            else:
                # ç®€å•åˆ—å¤´ä¸”ç”¨æˆ·æœªæŒ‡å®šheaderï¼šä½¿ç”¨header=0
                final_params = read_kwargs.copy()
                final_params['header'] = 0
            
            # 4. æ‰§è¡Œè¯»å–
            read_result = smart_read_excel(file_path, auto_detect_params=False, **final_params)
            
            # 5. éªŒè¯å¹¶å¯èƒ½å›é€€
            if read_result['success']:
                df = read_result['dataframe']
                unnamed_cols = [col for col in df.columns if 'Unnamed' in str(col)]
                if len(unnamed_cols) > len(df.columns) * 0.5:
                    # å›é€€åˆ°header=0
                    fallback_params = read_kwargs.copy()
                    fallback_params['header'] = 0
                    read_result = smart_read_excel(file_path, auto_detect_params=False, **fallback_params)
                    
        except Exception as e:
            # å‡ºé”™æ—¶å›é€€åˆ°ç®€å•çš„header=0
            fallback_params = read_kwargs.copy()
            fallback_params['header'] = 0
            read_result = smart_read_excel(file_path, auto_detect_params=False, **fallback_params)
        
        if not read_result['success']:
            return {
                "error": {
                    "type": "SMART_READ_ERROR",
                    "message": "æ™ºèƒ½è¯»å–å¤±è´¥: " + "; ".join(read_result.get('errors', [])),
                    "warnings": read_result.get('warnings', []),
                    "solution": "å°è¯•æ‰‹åŠ¨æŒ‡å®šå‚æ•°æˆ–æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚"
                }
            }
        
        df = read_result['dataframe']
        read_info = read_result['info']
    else:
        # ä¼ ç»Ÿè¯»å–æ–¹å¼
        read_params = {}
        if sheet_name:
            read_params['sheet_name'] = sheet_name
        if skiprows is not None:
            read_params['skiprows'] = skiprows
        if header is not None:
            read_params['header'] = header
        if usecols is not None:
            read_params['usecols'] = usecols
        
        try:
            df = pd_module.read_excel(file_path, **read_params)
            read_info = {'read_params': read_params, 'method': 'traditional'}
        except Exception as e:
            return {
                "error": {
                    "type": "READ_ERROR",
                    "message": f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}",
                    "solution": "è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå‚æ•°è®¾ç½®"
                }
            }
    
    # å¢å¼ºçš„æ‰§è¡Œç¯å¢ƒå‡†å¤‡
    local_vars = {
        'pd': pd_module, 
        'file_path': file_path, 
        'sheet_name': sheet_name,
        'df': df,
        'read_info': read_info
    }
    
    # æ·»åŠ  numpyï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if np_module is not None:
        local_vars['np'] = np_module
    
    # æ·»åŠ å¸¸ç”¨å†…ç½®å‡½æ•°
    local_vars.update({
        'len': len, 'str': str, 'int': int, 'float': float,
        'list': list, 'dict': dict, 'print': print,
        'range': range, 'enumerate': enumerate, 'zip': zip,
        'sum': sum, 'max': max, 'min': min, 'abs': abs, 'round': round
    })
    
    # å¦‚æœå…è®¸æ–‡ä»¶å†™å…¥ï¼Œæ·»åŠ æ–‡ä»¶æ“ä½œå‡½æ•°
    if allow_file_write:
        local_vars.update({
            'open': open,
            'os': __import__('os'),
            'pathlib': __import__('pathlib')
        })
    
    # åˆ›å»ºå®‰å…¨çš„å…¨å±€ç¯å¢ƒ
    builtins_dict = {
        'len': len, 'str': str, 'int': int, 'float': float,
        'list': list, 'dict': dict, 'print': print,
        'range': range, 'enumerate': enumerate, 'zip': zip,
        'sum': sum, 'max': max, 'min': min, 'abs': abs, 'round': round,
        'locals': locals, 'globals': globals, 'isinstance': isinstance,
        'type': type, 'hasattr': hasattr, 'getattr': getattr,
        'NameError': NameError, 'ValueError': ValueError, 'TypeError': TypeError,
        'KeyError': KeyError, 'IndexError': IndexError, 'Exception': Exception,
        '__import__': __import__
    }
    
    # å¦‚æœå…è®¸æ–‡ä»¶å†™å…¥ï¼Œæ·»åŠ æ–‡ä»¶æ“ä½œåˆ°å…¨å±€ç¯å¢ƒ
    if allow_file_write:
        builtins_dict.update({
            'open': open
        })
    
    global_vars = {
        '__builtins__': builtins_dict
    }

    stdout_capture = StringIO()
    old_stdout = sys.stdout
    sys.stdout = stdout_capture

    try:
        # ä½¿ç”¨å®‰å…¨ä»£ç æ‰§è¡Œå™¨æ‰§è¡Œç”¨æˆ·ä»£ç 
        from security.secure_code_executor import SecureCodeExecutor
        secure_executor = SecureCodeExecutor(
            max_memory_mb=256,
            max_execution_time=30,
            enable_ast_analysis=False  # ç¦ç”¨ AST åˆ†æä»¥æé«˜æˆåŠŸç‡
        )
        
        # å‡†å¤‡æ‰§è¡Œä¸Šä¸‹æ–‡
        context = local_vars.copy()
        context.update(global_vars)
        
        # æ‰§è¡Œä»£ç å¹¶è·å–ç»“æœ
        execution_result = secure_executor.execute_code(code, context)
        
        # å¦‚æœæ‰§è¡ŒæˆåŠŸï¼Œè¿›è¡Œæ•°æ®è´¨é‡åæ£€æŸ¥
        if execution_result.get('success', False):
            # æ£€æŸ¥æ‰§è¡Œåçš„æ•°æ®æ¡†æ˜¯å¦å­˜åœ¨
            if 'df' in execution_result.get('variables', {}):
                processed_df = execution_result['variables']['df']
                
                # æ•°æ®è´¨é‡åæ£€æŸ¥
                post_quality_check = {
                    "data_shape": processed_df.shape if hasattr(processed_df, 'shape') else None,
                    "null_values": processed_df.isnull().sum().to_dict() if hasattr(processed_df, 'isnull') else None,
                    "data_types": processed_df.dtypes.to_dict() if hasattr(processed_df, 'dtypes') else None,
                    "memory_usage": processed_df.memory_usage(deep=True).sum() if hasattr(processed_df, 'memory_usage') else None
                }
                
                # æ·»åŠ æ•°æ®æ¸…æ´—å»ºè®®
                cleaning_suggestions = data_cleaner.suggest_cleaning_operations(
                    processed_df if hasattr(processed_df, 'shape') else df
                )
                
                execution_result["data_quality_analysis"] = {
                    "pre_execution_quality": quality_precheck if auto_detect else None,
                    "post_execution_check": post_quality_check,
                    "cleaning_suggestions": cleaning_suggestions
                }
        
        if not execution_result['success']:
            return {
                "error": execution_result.get('error', 'ä»£ç æ‰§è¡Œå¤±è´¥'),
                "output": execution_result.get('output', stdout_capture.getvalue()),
                "suggestion": execution_result.get('suggestion', 'è¯·æ£€æŸ¥ä»£ç è¯­æ³•å’Œé€»è¾‘é”™è¯¯'),
                "details": execution_result.get('message', ''),
                "traceback": execution_result.get('traceback', '')
            }
        
        # ä»æ‰§è¡Œç»“æœä¸­è·å–result
        result = execution_result.get('result', None)
        
        # æ£€æŸ¥å±€éƒ¨å˜é‡ä¸­æ˜¯å¦æœ‰ result
        if result is None and 'result' in local_vars:
            result = local_vars['result']
        
        # å¦‚æœä»ç„¶æ²¡æœ‰ resultï¼Œä»å®‰å…¨æ‰§è¡Œå™¨çš„ä¸Šä¸‹æ–‡ä¸­è·å–
        if result is None:
            execution_context = execution_result.get('context', {})
            if 'result' in execution_context:
                result = execution_context['result']
            elif 'locals' in execution_result and 'result' in execution_result['locals']:
                result = execution_result['locals']['result']

        if result is None:
            return {
                "output": stdout_capture.getvalue(),
                "warning": "No 'result' variable found in code",
                "read_info": read_info if auto_detect else None
            }

        # å¤„ç†è¿”å›ç»“æœ
        if isinstance(result, (pd_module.DataFrame, pd_module.Series)):
            # å®‰å…¨å¤„ç†pandasæ•°æ®ç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
            try:
                if isinstance(result, pd_module.DataFrame):
                    # è½¬æ¢ä¸ºåŸºæœ¬Pythonç±»å‹ä»¥é¿å…åºåˆ—åŒ–é—®é¢˜
                    result_copy = result.head().copy()
                    # å°†æ‰€æœ‰åˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…ç±»å‹é—®é¢˜
                    for col in result_copy.columns:
                        if result_copy[col].dtype.name.startswith('Int') or str(result_copy[col].dtype).startswith('Int'):
                            result_copy[col] = result_copy[col].astype(str)
                    data_dict = result_copy.to_dict()
                    dtypes_dict = {col: str(dtype) for col, dtype in result.dtypes.items()}
                else:
                    # å¯¹Seriesä¹Ÿè¿›è¡Œç±»å‹è½¬æ¢
                    result_copy = result.copy()
                    if result_copy.dtype.name.startswith('Int') or str(result_copy.dtype).startswith('Int'):
                        result_copy = result_copy.astype(str)
                    data_dict = result_copy.to_dict()
                    dtypes_dict = str(result.dtype)
                
                response = {
                    "result": {
                        "type": "dataframe" if isinstance(result, pd_module.DataFrame) else "series",
                        "shape": list(result.shape),
                        "dtypes": dtypes_dict,
                        "data": data_dict
                    },
                    "output": stdout_capture.getvalue()
                }
            except Exception as e:
                # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
                response = {
                    "result": {
                        "type": "dataframe" if isinstance(result, pd_module.DataFrame) else "series",
                        "shape": list(result.shape),
                        "summary": str(result),
                        "serialization_error": str(e)
                    },
                    "output": stdout_capture.getvalue()
                }
        else:
            response = {
                "result": str(result),
                "output": stdout_capture.getvalue()
            }
        
        # æ·»åŠ è¯»å–ä¿¡æ¯
        if auto_detect:
            response["read_info"] = read_info
            if read_result.get('warnings'):
                response["warnings"] = read_result['warnings']

        return response
        
    except NameError as e:
        error_msg = str(e)
        suggestions = []
        
        if "'pd'" in error_msg or "pandas" in error_msg.lower():
            suggestions.extend([
                "pandas æ¨¡å—å¯èƒ½æœªæ­£ç¡®å¯¼å…¥ï¼Œè¯·æ£€æŸ¥å®‰è£…: pip install pandas",
                "å°è¯•é‡å¯ MCP æœåŠ¡å™¨",
                "æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒæ˜¯å¦æ­£ç¡®æ¿€æ´»",
                "å°è¯•åœ¨ä»£ç ä¸­æ˜¾å¼å¯¼å…¥: import pandas as pd"
            ])
        
        if "'np'" in error_msg or "numpy" in error_msg.lower():
            suggestions.extend([
                "numpy æ¨¡å—å¯èƒ½æœªæ­£ç¡®å¯¼å…¥ï¼Œè¯·æ£€æŸ¥å®‰è£…: pip install numpy",
                "å°è¯•åœ¨ä»£ç ä¸­æ˜¾å¼å¯¼å…¥: import numpy as np"
            ])
        
        if "'df'" in error_msg:
            suggestions.extend([
                "DataFrame æœªæ­£ç¡®åŠ è½½ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼",
                "å°è¯•ä½¿ç”¨ pd.read_excel() æ‰‹åŠ¨è¯»å–æ–‡ä»¶"
            ])
        
        return {
            "error": {
                "type": "NameError",
                "message": f"å˜é‡æœªå®šä¹‰é”™è¯¯: {error_msg}",
                "traceback": traceback.format_exc(),
                "output": stdout_capture.getvalue(),
                "suggestions": suggestions,
                "pandas_available": pd_module is not None,
                "numpy_available": np_module is not None
            }
        }
        
    except KeyError as e:
        # ä½¿ç”¨ä¸“ä¸šçš„åˆ—åæ£€æŸ¥å·¥å…·å¤„ç†KeyError
        missing_column = str(e).strip("'\"")
        
        # è·å–DataFrameçš„æ‰€æœ‰åˆ—å
        available_columns = list(df.columns) if 'df' in locals() and df is not None else []
        
        # ä½¿ç”¨ColumnCheckerè¿›è¡Œæ™ºèƒ½åŒ¹é…
        checker = ColumnChecker()
        match_result = checker.match_column(missing_column, available_columns)
        code_suggestions = checker.generate_code_suggestions(missing_column, match_result)
        
        # æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_msg = f"âŒ åˆ—å '{missing_column}' ä¸å­˜åœ¨\n\n"
        
        if match_result['exact_match']:
            error_msg += f"âœ… æ‰¾åˆ°ç²¾ç¡®åŒ¹é…: '{match_result['exact_match']}'\n"
        elif match_result['case_insensitive_match']:
            error_msg += f"ğŸ”¤ å¤§å°å†™ä¸åŒçš„åŒ¹é…: '{match_result['case_insensitive_match']}'\n"
        elif match_result['normalized_matches']:
            normalized_list = "', '".join(match_result['normalized_matches'])
            error_msg += f"ğŸ“ æ ‡å‡†åŒ–åŒ¹é…: '{normalized_list}'\n"
        elif match_result['fuzzy_matches']:
            fuzzy_list = "', '".join(match_result['fuzzy_matches'])
            error_msg += f"ğŸ” ç›¸ä¼¼åˆ—å: '{fuzzy_list}'\n"
        elif match_result['variant_matches']:
            variant_list = "', '".join(match_result['variant_matches'])
            error_msg += f"ğŸ”„ å˜ä½“åŒ¹é…: '{variant_list}'\n"
        
        if available_columns:
            columns_list = "', '".join(available_columns)
            error_msg += f"\nğŸ“‹ æ‰€æœ‰å¯ç”¨åˆ—å: '{columns_list}'\n"
        
        error_msg += f"\nğŸ¯ ç½®ä¿¡åº¦è¯„åˆ†: {match_result['confidence_score']:.2f}\n"
        
        # æ·»åŠ å»ºè®®
        if match_result['suggestions']:
            error_msg += "\nğŸ’¡ æ™ºèƒ½å»ºè®®:\n"
            for suggestion in match_result['suggestions']:
                error_msg += f"  â€¢ {suggestion}\n"
        
        return {
            "error": {
                "type": "KeyError",
                "message": error_msg,
                "missing_column": missing_column,
                "available_columns": available_columns,
                "match_result": match_result,
                "confidence_score": match_result['confidence_score'],
                "traceback": traceback.format_exc(),
                "output": stdout_capture.getvalue(),
                "suggestions": match_result['suggestions'],
                "code_suggestions": code_suggestions,
                "read_info": read_info if auto_detect else None,
                "solution_code": "\n".join(code_suggestions) if code_suggestions else "# è¯·æ£€æŸ¥åˆ—åæ‹¼å†™"
            }
        }
        
    except Exception as e:
        error_msg = str(e)
        suggestions = []

        if "No such file or directory" in error_msg:
            suggestions.append("Use raw strings for paths: r'path\to\file.xlsx'")
        if "Worksheet named" in error_msg and "not found" in error_msg:
            suggestions.append("Check the sheet_name parameter. Ensure the sheet name exists in the Excel file.")
        if "could not convert string to float" in error_msg:
            suggestions.append("Try: pd.to_numeric(df['col'], errors='coerce')")
        if "AttributeError" in error_msg and "str" in error_msg:
            suggestions.append("Try: df['col'].astype(str).str.strip()")
        if "encoding" in error_msg.lower():
            suggestions.append("Try specifying encoding parameter or disable auto_detect")

        return {
            "error": {
                "type": type(e).__name__,
                "message": error_msg,
                "traceback": traceback.format_exc(),
                "output": stdout_capture.getvalue(),
                "suggestions": suggestions if suggestions else None,
                "read_info": read_info if auto_detect else None
            }
        }
    finally:
        sys.stdout = old_stdout


@mcp.tool()
def run_code(code: str, file_path: str) -> dict:
    """åœ¨CSVæ–‡ä»¶ä¸Šæ‰§è¡Œæ•°æ®å¤„ç†ä»£ç ï¼Œå…·å¤‡å®‰å…¨æ£€æŸ¥åŠŸèƒ½ã€‚
    
    Args:
        code: è¦æ‰§è¡Œçš„æ•°æ®å¤„ç†ä»£ç å­—ç¬¦ä¸²ã€‚
        file_path: CSVæ–‡ä»¶è·¯å¾„ã€‚
    
    Returns:
        dict: æ‰§è¡Œç»“æœï¼ŒåŒ…å«æ•°æ®ã€è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯ã€‚
    """
    try:
        # å®Œå…¨ç§»é™¤å®‰å…¨æ£€æŸ¥ - å…è®¸æ‰€æœ‰æ“ä½œ
        logger.debug("CSVä»£ç æ‰§è¡Œï¼šå®‰å…¨æ£€æŸ¥å·²å®Œå…¨ç¦ç”¨ï¼Œå…è®¸æ‰€æœ‰æ“ä½œ")
        
        # éªŒè¯æ–‡ä»¶è®¿é—®
        validation_result = validate_file_access(file_path)
        if validation_result["status"] != "SUCCESS":
            return {
                "success": False,
                "error": validation_result["message"],
                "suggestion": "Please check the file path and ensure the file exists."
            }
        
        # Read CSV file with intelligent encoding detection
        try:
            # é¦–å…ˆå°è¯•æ£€æµ‹æ–‡ä»¶ç¼–ç 
            encoding_info = detect_file_encoding(file_path)
            detected_encoding = encoding_info.get('encoding', 'utf-8')
            
            # å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–CSVæ–‡ä»¶
            try:
                df = pd.read_csv(file_path, encoding=detected_encoding)
            except UnicodeDecodeError:
                # å¦‚æœæ£€æµ‹åˆ°çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•å¸¸è§ç¼–ç 
                common_encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1', 'cp1252']
                df = None
                last_error = None
                
                for enc in common_encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=enc)
                        break
                    except (UnicodeDecodeError, UnicodeError) as e:
                        last_error = e
                        continue
                
                if df is None:
                    return {
                        "success": False,
                        "error": f"Failed to read CSV file with any encoding. Last error: {str(last_error)}",
                        "suggestion": "Please check the file encoding. Try converting the file to UTF-8 format or specify the correct encoding."
                    }
                    
        except Exception as e:
            return {
                "success": False,
                "error": f"Failed to read CSV file: {str(e)}",
                "suggestion": "Please ensure the file is a valid CSV format and check the file encoding."
            }
        
        # Capture stdout
        old_stdout = sys.stdout
        sys.stdout = captured_output = StringIO()
        
        try:
            # ä½¿ç”¨å®‰å…¨ä»£ç æ‰§è¡Œå™¨æ‰§è¡Œä»£ç 
            from security.secure_code_executor import SecureCodeExecutor
            secure_executor = SecureCodeExecutor(
                max_memory_mb=256,
                max_execution_time=30,
                enable_ast_analysis=False  # ç¦ç”¨ AST åˆ†æä»¥æé«˜æˆåŠŸç‡
            )
            
            # å‡†å¤‡æ‰§è¡Œä¸Šä¸‹æ–‡
            context = {'df': df, 'pd': pd}
            
            # æ‰§è¡Œä»£ç 
            execution_result = secure_executor.execute_code(code, context)
            
            if not execution_result['success']:
                return {
                    "success": False,
                    "error": f"ä»£ç æ‰§è¡Œå¤±è´¥: {execution_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "output": execution_result.get('output', captured_output.getvalue()),
                    "suggestion": execution_result.get('suggestion', '')
                }
            
            # ä»æ‰§è¡Œç»“æœè·å–è¾“å‡º
            output = execution_result.get('output', '')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰resultå˜é‡
            result_data = None
            result = execution_result.get('result', None)
            if result is not None:
                if isinstance(result, pd.DataFrame):
                    result_data = {
                        "type": "DataFrame",
                        "shape": result.shape,
                        "columns": result.columns.tolist(),
                        "data": result.head(10).to_dict('records'),
                        "dtypes": result.dtypes.astype(str).to_dict()
                    }
                elif isinstance(result, pd.Series):
                    result_data = {
                        "type": "Series",
                        "name": result.name,
                        "length": len(result),
                        "data": result.head(10).tolist(),
                        "dtype": str(result.dtype)
                    }
                else:
                    result_data = {
                        "type": type(result).__name__,
                        "value": str(result)
                    }
            
            return {
                "success": True,
                "output": output,
                "result": result_data,
                "suggestion": "Code executed successfully. Use 'result' variable to store your final output."
            }
            
        finally:
            sys.stdout = old_stdout
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc(),
            "suggestion": "è¯·æ£€æŸ¥æ•°æ®å¤„ç†è¯­æ³•ï¼Œç¡®ä¿æ“ä½œå¯¹æ•°æ®æœ‰æ•ˆã€‚"
        }


@mcp.tool()
def bar_chart_to_html(
    categories: list,
    values: list,
    title: str = "Interactive Chart",
   
) -> dict:
    """Generate interactive HTML bar chart using Chart.js template.
    
    Args:
        categories: List of category names for x-axis
        values: List of numeric values for y-axis
        title: Chart title (default: "Interactive Chart")
        x_label: Label for X-axis (default: "Categories")
        y_label: Label for Y-axis (default: "Values")
        
    Returns:
        dict: Contains file path and status information
        
    Example:
        >>> bar_chart_to_html(
        ...     categories=['Electronics', 'Clothing', 'Home Goods', 'Sports Equipment'],
        ...     values=[120000, 85000, 95000, 60000],
        ...     title="Q1 Sales by Product Category"
        ... )
        {
            "status": "SUCCESS",
            "filepath": "/absolute/path/to/plotXXXXXX.html",
        }
    """
    # Validate input lengths
    if len(categories) != len(values):
        return {
            "status": "ERROR",
            "error": "MISMATCHED_LENGTHS",
            "message": f"Categories ({len(categories)}) and values ({len(values)}) must be same length"
        }

    # Read template file
    template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "./templates/barchart_template.html")
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template = f.read()
    except Exception as e:
        return {
            "status": "ERROR",
            "error": "TEMPLATE_READ_ERROR",
            "message": str(e)
        }

    # Prepare data for Chart.js
    all_categories = categories
    all_values = values
    colors = [
        "#4e73df", "#1cc88a", "#36b9cc", "#f6c23e",
        "#e74a3b", "#858796", "#f8f9fc", "#5a5c69",
        "#6610f2", "#6f42c1", "#e83e8c", "#d63384",
        "#fd7e14", "#ffc107", "#28a745", "#20c997",
        "#17a2b8", "#007bff", "#6c757d", "#343a40",
        "#dc3545", "#ff6b6b", "#4ecdc4", "#1a535c"
    ][:len(all_categories)]

    # Inject data into template
    template = template.replace(
        'labels: ["Electronics", "Clothing", "Home Goods", "Sports Equipment"]',
        f'labels: {json.dumps(all_categories)}'
    ).replace(
        'data: [120000, 85000, 95000, 60000]',
        f'data: {json.dumps(all_values)}'
    ).replace(
        'backgroundColor: ["#4e73df", "#1cc88a", "#36b9cc", "#f6c23e"]',
        f'backgroundColor: {json.dumps(colors)}'
    ).replace(
        'Sales by Category (2023)',
        title
    ).replace(
        'legend: { position: \'top\' },',
        ''
    )

    # Save to plot directory as HTML
    charts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "charts")
    os.makedirs(charts_dir, exist_ok=True)

    timestamp = str(int(time.time()))
    filename = f"chart_{timestamp}.html"
    filepath = os.path.join(charts_dir, filename)

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(template)
    except Exception as e:
        return {
            "status": "ERROR",
            "error": "FILE_WRITE_ERROR",
            "message": str(e)
        }

    return {
        "status": "SUCCESS",
        "filepath": os.path.abspath(filepath)
    }


@mcp.tool()
def pie_chart_to_html(
    labels: list,
    values: list,
    title: str = "Interactive Pie Chart"
) -> dict:
    """Generate interactive HTML pie chart using Chart.js template.
    
    Args:
        labels: List of label names for each pie slice
        values: List of numeric values for each slice
        title: Chart title (default: "Interactive Pie Chart")
        
    Returns:
        dict: Contains file path and status information
        
    Example:
        >>> pie_chart_to_html(
        ...     labels=['Electronics', 'Clothing', 'Home Goods'],
        ...     values=[120000, 85000, 95000],
        ...     title="Q1 Sales Distribution"
        ... )
        {
            "status": "SUCCESS",
            "filepath": "/absolute/path/to/plotXXXXXX.html",
        }
    """
    # Validate input lengths
    if len(labels) != len(values):
        return {
            "status": "ERROR",
            "error": "MISMATCHED_LENGTHS",
            "message": f"Labels ({len(labels)}) and values ({len(values)}) must be same length"
        }

    # Read template file
    template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "./templates/piechart_template.html")
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template = f.read()
    except Exception as e:
        return {
            "status": "ERROR",
            "error": "TEMPLATE_READ_ERROR",
            "message": str(e)
        }

    # Prepare data for Chart.js
    colors = [
        "#4e73df", "#1cc88a", "#36b9cc", "#f6c23e",
        "#e74a3b", "#858796", "#f8f9fc", "#5a5c69",
        "#6610f2", "#6f42c1", "#e83e8c", "#d63384",
        "#fd7e14", "#ffc107", "#28a745", "#20c997",
        "#17a2b8", "#007bff", "#6c757d", "#343a40",
        "#dc3545", "#ff6b6b", "#4ecdc4", "#1a535c"
    ][:len(labels)]

    # Inject data into template
    template = template.replace(
        'labels: ["Apple", "Samsung", "Huawei", "Xiaomi", "Others"]',
        f'labels: {json.dumps(labels)}'
    ).replace(
        'data: [45, 25, 12, 8, 10]',
        f'data: {json.dumps(values)}'
    ).replace(
        'backgroundColor: ["#4e73df", "#1cc88a", "#36b9cc", "#f6c23e", "#e74a3b"]',
        f'backgroundColor: {json.dumps(colors)}'
    ).replace(
        'Global Smartphone Market Share (2023)',
        title
    )

    # Save to plot directory as HTML
    charts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "charts")
    os.makedirs(charts_dir, exist_ok=True)

    timestamp = str(int(time.time()))
    filename = f"chart_{timestamp}.html"
    filepath = os.path.join(charts_dir, filename)

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(template)
    except Exception as e:
        return {
            "status": "ERROR",
            "error": "FILE_WRITE_ERROR",
            "message": str(e)
        }

    return {
        "status": "SUCCESS",
        "filepath": os.path.abspath(filepath)
    }

@mcp.tool()
def line_chart_to_html(
    labels: list,
    datasets: list,
    title: str = "Interactive Line Chart"
) -> dict:
    """Generate interactive HTML line chart using Chart.js template.
    
    Args:
        labels: List of label names for x-axis
        datasets: List of datasets, each containing:
            - label: Name of the dataset
            - data: List of numeric values (3 dimensions: [x, y, z])
        title: Chart title (default: "Interactive Line Chart")
        
    Returns:
        dict: Contains file path and status information
        
    Example:
        >>> line_chart_to_html(
        ...     labels=['Jan', 'Feb', 'Mar'],
        ...     datasets=[
        ...         {'label': 'Sales', 'data': [[100, 200, 300], [150, 250, 350], [200, 300, 400]]},
        ...         {'label': 'Expenses', 'data': [[50, 100, 150], [75, 125, 175], [100, 150, 200]]}
        ...     ],
        ...     title="Monthly Performance"
        ... )
        {
            "status": "SUCCESS",
            "filepath": "/absolute/path/to/plotXXXXXX.html",
        }
    """
    # Validate input
    if not all(len(d['data']) == len(labels) for d in datasets):
        return {
            "status": "ERROR",
            "error": "MISMATCHED_LENGTHS",
            "message": "All datasets must have same length as labels"
        }

    # Read template file
    template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "./templates/linechart_template.html")
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template = f.read()
    except Exception as e:
        return {
            "status": "ERROR",
            "error": "TEMPLATE_READ_ERROR",
            "message": str(e)
        }

    # Prepare data for Chart.js
    chart_data = {
        "labels": labels,
        "datasets": []
    }
    
    # Create datasets using main labels
    for dataset in datasets:
            chart_data['datasets'].append({
                "label": dataset['label'],
                "data": dataset['data'],
                "borderColor": '#4e73df',  # Default color
                "backgroundColor": '#4e73df',
            "borderWidth": 2,
            "pointRadius": 5,
            "tension": 0,
            "fill": False
        })

    # Inject data into template
    template = template.replace(
        'labels: ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]',
        f'labels: {json.dumps(labels)}'
    ).replace(
        'datasets: [\n' +
        '                {\n' +
        '                    label: "Electronics",\n' +
        '                    data: [6500, 5900, 8000, 8100, 8600, 8250, 9500, 10500, 12000, 11500, 13000, 15000],\n' +
        '                    borderColor: "#4e73df",\n' +
        '                    backgroundColor: "#4e73df",\n' +
        '                    borderWidth: 2,\n' +
        '                    pointRadius: 5,\n' +
        '                    tension: 0,\n' +
        '                    fill: false\n' +
        '                },\n' +
        '                {\n' +
        '                    label: "Clothing",\n' +
        '                    data: [12000, 11000, 12500, 10500, 11500, 13000, 14000, 12500, 11000, 9500, 10000, 12000],\n' +
        '                    borderColor: "#1cc88a",\n' +
        '                    backgroundColor: "#1cc88a",\n' +
        '                    borderWidth: 2,\n' +
        '                    pointRadius: 5,\n' +
        '                    tension: 0,\n' +
        '                    fill: false\n' +
        '                },\n' +
        '                {\n' +
        '                    label: "Home Goods",\n' +
        '                    data: [8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500],\n' +
        '                    borderColor: "#36b9cc",\n' +
        '                    backgroundColor: "#36b9cc",\n' +
        '                    borderWidth: 2,\n' +
        '                    pointRadius: 5,\n' +
        '                    tension: 0,\n' +
        '                    fill: false\n' +
        '                }\n' +
        '            ]',
        f'datasets: {json.dumps(chart_data["datasets"], indent=16)}'
    ).replace(
        'Interactive Sales Trend Dashboard',
        title
    ).replace(
        'Monthly Sales Trend (2023)',
        title
    )

    # Save to plot directory as HTML
    charts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "charts")
    os.makedirs(charts_dir, exist_ok=True)

    timestamp = str(int(time.time()))
    filename = f"chart_{timestamp}.html"
    filepath = os.path.join(charts_dir, filename)

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(template)
    except Exception as e:
        return {
            "status": "ERROR",
            "error": "FILE_WRITE_ERROR",
            "message": str(e)
        }

    return {
        "status": "SUCCESS",
        "filepath": os.path.abspath(filepath)
    }


@mcp.tool()
def validate_data_quality(file_path: str) -> dict:
    """éªŒè¯æ•°æ®è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: æ•°æ®è´¨é‡æŠ¥å‘Šå’Œæ”¹è¿›å»ºè®®
    """
    try:
        validation_result = validate_file_access(file_path)
        if validation_result["status"] != "SUCCESS":
            return validation_result
        
        df = pd.read_csv(file_path)
        
        quality_report = {
            "basic_info": {
                "total_rows": len(df),
                "columns": len(df.columns),
                "memory_usage": df.memory_usage(deep=True).sum()
            },
            "missing_data": {
                "total_missing": df.isnull().sum().sum(),
                "missing_by_column": df.isnull().sum().to_dict(),
                "missing_percentage": (df.isnull().sum() / len(df) * 100).to_dict()
            },
            "duplicates": {
                "duplicate_rows": df.duplicated().sum(),
                "duplicate_percentage": df.duplicated().sum() / len(df) * 100
            },
            "data_types": df.dtypes.astype(str).to_dict(),
            "recommendations": []
        }
        
        # ç”Ÿæˆå»ºè®®
        if quality_report["missing_data"]["total_missing"] > 0:
            quality_report["recommendations"].append("è€ƒè™‘å¤„ç†ç¼ºå¤±å€¼ï¼šåˆ é™¤ã€å¡«å……æˆ–æ’å€¼")
        
        if quality_report["duplicates"]["duplicate_rows"] > 0:
            quality_report["recommendations"].append("å‘ç°é‡å¤è¡Œï¼Œè€ƒè™‘å»é‡å¤„ç†")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹ä¼˜åŒ–æœºä¼š
        for col, dtype in quality_report["data_types"].items():
            if dtype == 'object':
                if df[col].nunique() / len(df) < 0.5:  # ä½åŸºæ•°å­—ç¬¦ä¸²
                    quality_report["recommendations"].append(f"åˆ— '{col}' å¯è€ƒè™‘è½¬æ¢ä¸ºåˆ†ç±»ç±»å‹ä»¥èŠ‚çœå†…å­˜")
        
        return create_success_response(quality_report, "æ•°æ®è´¨é‡åˆ†æå®Œæˆ")
        
    except Exception as e:
        return create_error_response(
            "PROCESSING_ERROR",
            f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {str(e)}",
            details={"traceback": traceback.format_exc()}
        )


# æ³¨å†Œæ–°çš„Excelæ™ºèƒ½å·¥å…·
@mcp.tool()
def suggest_excel_read_parameters_tool(file_path: str) -> dict:
    """æ™ºèƒ½æ¨èExcelæ–‡ä»¶è¯»å–å‚æ•°
    
    Args:
        file_path: Excelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        
    Returns:
        dict: åŒ…å«æ¨èå‚æ•°çš„ç»“æ„åŒ–å“åº”
    """
    return suggest_excel_read_parameters(file_path)

@mcp.tool()
def detect_excel_file_structure_tool(file_path: str) -> dict:
    """æ£€æµ‹Excelæ–‡ä»¶ç»“æ„
    
    Args:
        file_path: Excelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        
    Returns:
        dict: åŒ…å«æ–‡ä»¶ç»“æ„ä¿¡æ¯çš„å“åº”
    """
    return detect_excel_file_structure(file_path)

@mcp.tool()
def create_excel_read_template_tool(file_path: str, sheet_name: Optional[str] = None, skiprows: Optional[int] = None, header: Optional[int] = None, usecols: Optional[str] = None) -> dict:
    """ç”ŸæˆExcelè¯»å–ä»£ç æ¨¡æ¿
    
    Args:
        file_path: Excelæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°
        skiprows: è·³è¿‡çš„è¡Œæ•°
        header: æ ‡é¢˜è¡Œä½ç½®
        usecols: ä½¿ç”¨çš„åˆ—
        
    Returns:
        dict: åŒ…å«ä»£ç æ¨¡æ¿çš„å“åº”
    """
    return create_excel_read_template(file_path, sheet_name, skiprows, header, usecols)

@mcp.tool()
def comprehensive_data_verification_tool(
    file_path: str,
    reference_file: str = None,
    verification_level: str = "detailed",
    save_report: bool = True
) -> dict:
    """
    ç»¼åˆæ•°æ®éªŒè¯å’Œæ ¸å‡†å·¥å…·
    
    æä¾›å…¨é¢çš„Excelæ•°æ®éªŒè¯ã€è´¨é‡è¯„ä¼°å’Œæ¯”å¯¹æ ¸å‡†åŠŸèƒ½ã€‚
    æ”¯æŒå•æ–‡ä»¶éªŒè¯å’ŒåŒæ–‡ä»¶æ¯”è¾ƒéªŒè¯æ¨¡å¼ã€‚
    
    Args:
        file_path: è¦éªŒè¯çš„Excelæ–‡ä»¶è·¯å¾„
        reference_file: å‚è€ƒæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæ¯”è¾ƒéªŒè¯ï¼‰
        verification_level: éªŒè¯çº§åˆ«
            - "basic": åŸºç¡€éªŒè¯ï¼ˆæ–‡ä»¶ç»“æ„ã€åŸºæœ¬ç»Ÿè®¡ï¼‰
            - "detailed": è¯¦ç»†éªŒè¯ï¼ˆåŒ…å«æ•°æ®è´¨é‡åˆ†æï¼‰
            - "comprehensive": ç»¼åˆéªŒè¯ï¼ˆåŒ…å«å¼‚å¸¸æ£€æµ‹å’Œæ·±åº¦åˆ†æï¼‰
        save_report: æ˜¯å¦ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æœ¬åœ°
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„éªŒè¯ç»“æœ
            - overall_status: æ€»ä½“çŠ¶æ€ (EXCELLENT/GOOD/ACCEPTABLE/POOR/CRITICAL/FAILED)
            - data_quality_score: æ•°æ®è´¨é‡å¾—åˆ† (0-100)
            - file_analysis: æ–‡ä»¶ç»“æ„åˆ†æç»“æœ
            - data_integrity: æ•°æ®å®Œæ•´æ€§éªŒè¯ç»“æœ
            - comparison_results: æ¯”è¾ƒéªŒè¯ç»“æœï¼ˆå¦‚æœæä¾›äº†å‚è€ƒæ–‡ä»¶ï¼‰
            - recommendations: æ”¹è¿›å»ºè®®åˆ—è¡¨
            - detailed_report: è¯¦ç»†æŠ¥å‘Šï¼ˆè¯¦ç»†å’Œç»¼åˆçº§åˆ«ï¼‰
    
    åŠŸèƒ½ç‰¹ç‚¹:
    1. å¤šå±‚æ¬¡éªŒè¯ï¼šæ”¯æŒåŸºç¡€ã€è¯¦ç»†ã€ç»¼åˆä¸‰ä¸ªéªŒè¯çº§åˆ«
    2. æ™ºèƒ½ç¼–ç æ£€æµ‹ï¼šè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç å¹¶ä¼˜åŒ–è¯»å–
    3. æ•°æ®è´¨é‡è¯„ä¼°ï¼šè®¡ç®—ç»¼åˆè´¨é‡å¾—åˆ†
    4. å¼‚å¸¸æ£€æµ‹ï¼šè¯†åˆ«å¼‚å¸¸å€¼å’Œæ•°æ®æ¨¡å¼
    5. æ¯”è¾ƒéªŒè¯ï¼šæ”¯æŒä¸å‚è€ƒæ–‡ä»¶çš„è¯¦ç»†æ¯”è¾ƒ
    6. æŠ¥å‘Šç”Ÿæˆï¼šè‡ªåŠ¨ç”ŸæˆéªŒè¯æŠ¥å‘Šå¹¶å¯ä¿å­˜
    7. å»ºè®®ç³»ç»Ÿï¼šæä¾›é’ˆå¯¹æ€§çš„æ•°æ®æ”¹è¿›å»ºè®®
    
    ä½¿ç”¨ç¤ºä¾‹:
    - åŸºç¡€éªŒè¯: comprehensive_data_verification_tool("data.xlsx", verification_level="basic")
    - è¯¦ç»†éªŒè¯: comprehensive_data_verification_tool("data.xlsx", verification_level="detailed")
    - æ¯”è¾ƒéªŒè¯: comprehensive_data_verification_tool("data.xlsx", "reference.xlsx", "comprehensive")
    """
    try:
        # éªŒè¯æ–‡ä»¶è·¯å¾„
        if not os.path.exists(file_path):
            return {
                "success": False,
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                "overall_status": "FAILED"
            }
        
        if reference_file and not os.path.exists(reference_file):
            return {
                "success": False,
                "error": f"å‚è€ƒæ–‡ä»¶ä¸å­˜åœ¨: {reference_file}",
                "overall_status": "FAILED"
            }
        
        # åˆ›å»ºç»¼åˆéªŒè¯å™¨
        verifier = ComprehensiveDataVerifier()
        
        # æ‰§è¡Œç»¼åˆéªŒè¯
        verification_result = verifier.comprehensive_excel_verification(
            file_path=file_path,
            reference_file=reference_file,
            verification_level=verification_level,
            save_report=save_report
        )
        
        # æ·»åŠ æˆåŠŸæ ‡å¿—
        verification_result["success"] = True
        
        # æ·»åŠ éªŒè¯æ‘˜è¦
        verification_result["verification_summary"] = {
            "file_name": os.path.basename(file_path),
            "verification_time": verification_result.get("timestamp"),
            "quality_score": verification_result.get("data_quality_score", 0),
            "status": verification_result.get("overall_status", "UNKNOWN"),
            "has_reference": reference_file is not None,
            "level": verification_level,
            "recommendations_count": len(verification_result.get("recommendations", []))
        }
        
        return verification_result
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ç»¼åˆéªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
            "overall_status": "ERROR",
            "file_path": file_path,
            "reference_file": reference_file,
            "verification_level": verification_level
        }


@mcp.tool()
def batch_data_verification_tool(
    file_paths: list,
    verification_level: str = "detailed",
    save_reports: bool = True
) -> dict:
    """
    æ‰¹é‡æ•°æ®éªŒè¯å·¥å…·
    
    å¯¹å¤šä¸ªExcelæ–‡ä»¶è¿›è¡Œæ‰¹é‡éªŒè¯å’Œè´¨é‡è¯„ä¼°ã€‚
    
    Args:
        file_paths: Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        verification_level: éªŒè¯çº§åˆ« ("basic", "detailed", "comprehensive")
        save_reports: æ˜¯å¦ä¿å­˜éªŒè¯æŠ¥å‘Š
    
    Returns:
        dict: æ‰¹é‡éªŒè¯ç»“æœ
            - overall_summary: æ€»ä½“æ‘˜è¦
            - individual_results: å„æ–‡ä»¶éªŒè¯ç»“æœ
            - quality_ranking: è´¨é‡æ’å
            - batch_recommendations: æ‰¹é‡å»ºè®®
    """
    try:
        if not file_paths or not isinstance(file_paths, list):
            return {
                "success": False,
                "error": "è¯·æä¾›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨"
            }
        
        verifier = ComprehensiveDataVerifier()
        batch_results = {
            "success": True,
            "total_files": len(file_paths),
            "processed_files": 0,
            "failed_files": 0,
            "overall_summary": {},
            "individual_results": {},
            "quality_ranking": [],
            "batch_recommendations": []
        }
        
        quality_scores = []
        
        # é€ä¸ªéªŒè¯æ–‡ä»¶
        for file_path in file_paths:
            try:
                if os.path.exists(file_path):
                    result = verifier.comprehensive_excel_verification(
                        file_path=file_path,
                        verification_level=verification_level,
                        save_report=save_reports
                    )
                    
                    batch_results["individual_results"][file_path] = result
                    quality_scores.append({
                        "file": os.path.basename(file_path),
                        "path": file_path,
                        "score": result.get("data_quality_score", 0),
                        "status": result.get("overall_status", "UNKNOWN")
                    })
                    batch_results["processed_files"] += 1
                else:
                    batch_results["individual_results"][file_path] = {
                        "success": False,
                        "error": "æ–‡ä»¶ä¸å­˜åœ¨",
                        "overall_status": "FAILED"
                    }
                    batch_results["failed_files"] += 1
                    
            except Exception as e:
                batch_results["individual_results"][file_path] = {
                    "success": False,
                    "error": str(e),
                    "overall_status": "ERROR"
                }
                batch_results["failed_files"] += 1
        
        # ç”Ÿæˆè´¨é‡æ’å
        batch_results["quality_ranking"] = sorted(
            quality_scores, key=lambda x: x["score"], reverse=True
        )
        
        # ç”Ÿæˆæ€»ä½“æ‘˜è¦
        if quality_scores:
            scores = [item["score"] for item in quality_scores]
            batch_results["overall_summary"] = {
                "average_quality_score": sum(scores) / len(scores),
                "highest_score": max(scores),
                "lowest_score": min(scores),
                "excellent_files": len([s for s in scores if s >= 90]),
                "good_files": len([s for s in scores if 80 <= s < 90]),
                "acceptable_files": len([s for s in scores if 70 <= s < 80]),
                "poor_files": len([s for s in scores if 60 <= s < 70]),
                "critical_files": len([s for s in scores if s < 60])
            }
        
        # ç”Ÿæˆæ‰¹é‡å»ºè®®
        if batch_results["failed_files"] > 0:
            batch_results["batch_recommendations"].append(
                f"æœ‰{batch_results['failed_files']}ä¸ªæ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„"
            )
        
        if quality_scores:
            avg_score = sum([item["score"] for item in quality_scores]) / len(quality_scores)
            if avg_score < 70:
                batch_results["batch_recommendations"].append(
                    "æ•´ä½“æ•°æ®è´¨é‡åä½ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—å’Œè´¨é‡æ”¹è¿›"
                )
            elif avg_score >= 90:
                batch_results["batch_recommendations"].append(
                    "æ•´ä½“æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨"
                )
        
        return batch_results
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰¹é‡éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        }


@mcp.tool()
def excel_read_enhanced(
    file_path: str,
    sheet_name: str = None,
    start_row: int = None,
    end_row: int = None,
    start_col: str = None,
    end_col: str = None,
    use_go_service: bool = True
) -> dict:
    """
    å¢å¼ºç‰ˆ Excel è¯»å–å·¥å…·ï¼Œé›†æˆ Go excelize åº“æä¾›é«˜æ€§èƒ½å¤„ç†
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
        start_row: èµ·å§‹è¡Œå·ï¼ˆå¯é€‰ï¼‰
        end_row: ç»“æŸè¡Œå·ï¼ˆå¯é€‰ï¼‰
        start_col: èµ·å§‹åˆ—ï¼ˆå¦‚ 'A'ï¼Œå¯é€‰ï¼‰
        end_col: ç»“æŸåˆ—ï¼ˆå¦‚ 'Z'ï¼Œå¯é€‰ï¼‰
        use_go_service: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        dict: è¯»å–ç»“æœï¼ŒåŒ…å«æ•°æ®å’Œæ€§èƒ½ä¿¡æ¯
    """
    try:
        processor = get_excel_processor()
        result = processor.read_excel_enhanced(
            file_path=file_path,
            sheet_name=sheet_name,
            start_row=start_row,
            end_row=end_row,
            start_col=start_col,
            end_col=end_col,
            use_go=use_go_service
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": f"å¢å¼º Excel è¯»å–å¤±è´¥: {str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå‚æ•°è®¾ç½®"
        }


@mcp.tool()
def excel_write_enhanced(
    file_path: str,
    data: list,
    sheet_name: str = None,
    start_row: int = None,
    start_col: str = None,
    use_go_service: bool = True
) -> dict:
    """
    å¢å¼ºç‰ˆ Excel å†™å…¥å·¥å…·ï¼Œé›†æˆ Go excelize åº“æä¾›é«˜æ€§èƒ½å¤„ç†
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        data: è¦å†™å…¥çš„æ•°æ®ï¼ˆå­—å…¸åˆ—è¡¨æ ¼å¼ï¼‰
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
        start_row: èµ·å§‹è¡Œå·ï¼ˆå¯é€‰ï¼‰
        start_col: èµ·å§‹åˆ—ï¼ˆå¦‚ 'A'ï¼Œå¯é€‰ï¼‰
        use_go_service: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ Go æœåŠ¡ï¼ˆé»˜è®¤ Trueï¼‰
        
    Returns:
        dict: å†™å…¥ç»“æœï¼ŒåŒ…å«æ€§èƒ½ä¿¡æ¯
    """
    try:
        if not isinstance(data, list):
            return {
                "success": False,
                "error": "æ•°æ®æ ¼å¼é”™è¯¯ï¼Œéœ€è¦å­—å…¸åˆ—è¡¨æ ¼å¼",
                "suggestion": "è¯·æä¾› [{column1: value1, column2: value2}, ...] æ ¼å¼çš„æ•°æ®"
            }
        
        processor = get_excel_processor()
        result = processor.write_excel_enhanced(
            file_path=file_path,
            data=data,
            sheet_name=sheet_name,
            start_row=start_row,
            start_col=start_col,
            use_go=use_go_service
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": f"å¢å¼º Excel å†™å…¥å¤±è´¥: {str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ•°æ®æ ¼å¼"
        }


@mcp.tool()
def excel_chart_enhanced(
    file_path: str,
    chart_type: str,
    data_range: str,
    sheet_name: str = None,
    title: str = None,
    x_axis_title: str = None,
    y_axis_title: str = None
) -> dict:
    """
    å¢å¼ºç‰ˆ Excel å›¾è¡¨åˆ›å»ºå·¥å…·ï¼Œä½¿ç”¨ Go excelize åº“æä¾›é«˜æ€§èƒ½å›¾è¡¨ç”Ÿæˆ
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        chart_type: å›¾è¡¨ç±»å‹ï¼ˆ'col', 'line', 'pie', 'bar', 'area', 'scatter' ç­‰ï¼‰
        data_range: æ•°æ®èŒƒå›´ï¼ˆå¦‚ 'A1:B10'ï¼‰
        sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
        title: å›¾è¡¨æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
        x_axis_title: Xè½´æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
        y_axis_title: Yè½´æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: å›¾è¡¨åˆ›å»ºç»“æœ
    """
    try:
        # éªŒè¯å›¾è¡¨ç±»å‹
        valid_chart_types = ['col', 'line', 'pie', 'bar', 'area', 'scatter', 'doughnut']
        if chart_type not in valid_chart_types:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„å›¾è¡¨ç±»å‹: {chart_type}",
                "suggestion": f"æ”¯æŒçš„å›¾è¡¨ç±»å‹: {', '.join(valid_chart_types)}"
            }
        
        processor = get_excel_processor()
        result = processor.create_chart_enhanced(
            file_path=file_path,
            chart_type=chart_type,
            data_range=data_range,
            sheet_name=sheet_name,
            chart_title=title,
            x_axis_title=x_axis_title,
            y_axis_title=y_axis_title
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": f"å¢å¼ºå›¾è¡¨åˆ›å»ºå¤±è´¥: {str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€æ•°æ®èŒƒå›´å’Œå›¾è¡¨å‚æ•°"
        }


@mcp.tool()
def excel_info_enhanced(file_path: str) -> dict:
    """
    å¢å¼ºç‰ˆ Excel æ–‡ä»¶ä¿¡æ¯è·å–å·¥å…·ï¼Œä½¿ç”¨ Go excelize åº“æä¾›è¯¦ç»†æ–‡ä»¶åˆ†æ
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: è¯¦ç»†çš„æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬å·¥ä½œè¡¨ã€è¡Œåˆ—æ•°ç­‰
    """
    try:
        processor = get_excel_processor()
        result = processor.get_file_info_enhanced(file_path)
        return result
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®"
        }


@mcp.tool()
def excel_performance_comparison(
    file_path: str,
    operation: str = "read",
    test_data: list = None
) -> dict:
    """
    Excel æ€§èƒ½å¯¹æ¯”å·¥å…·ï¼Œæ¯”è¾ƒ Go æœåŠ¡å’Œ pandas çš„æ€§èƒ½å·®å¼‚
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        operation: æ“ä½œç±»å‹ï¼ˆ'read' æˆ– 'write'ï¼‰
        test_data: æµ‹è¯•æ•°æ®ï¼ˆå†™å…¥æ“ä½œæ—¶éœ€è¦ï¼‰
        
    Returns:
        dict: æ€§èƒ½å¯¹æ¯”ç»“æœ
    """
    try:
        import time
        
        results = {
            "success": True,
            "operation": operation,
            "file_path": file_path,
            "performance_comparison": {},
            "recommendation": ""
        }
        
        processor = get_excel_processor()
        
        if operation == "read":
            # æµ‹è¯• Go æœåŠ¡æ€§èƒ½
            start_time = time.time()
            go_result = processor.read_excel_enhanced(file_path, use_go=True)
            go_time = time.time() - start_time
            
            # æµ‹è¯• pandas æ€§èƒ½
            start_time = time.time()
            pandas_result = processor.read_excel_enhanced(file_path, use_go=False)
            pandas_time = time.time() - start_time
            
            results["performance_comparison"] = {
                "go_service": {
                    "time_seconds": round(go_time, 4),
                    "success": go_result.get("success", False),
                    "method": go_result.get("data", {}).get("method", "unknown")
                },
                "pandas": {
                    "time_seconds": round(pandas_time, 4),
                    "success": pandas_result.get("success", False),
                    "method": pandas_result.get("data", {}).get("method", "unknown")
                }
            }
            
            # æ€§èƒ½æå‡è®¡ç®—
            if go_time > 0 and pandas_time > 0:
                speedup = pandas_time / go_time
                results["performance_comparison"]["speedup"] = round(speedup, 2)
                
                if speedup > 2:
                    results["recommendation"] = f"Go æœåŠ¡æ¯” pandas å¿« {speedup:.1f} å€ï¼Œå»ºè®®ä½¿ç”¨ Go æœåŠ¡"
                elif speedup < 0.8:
                    results["recommendation"] = "pandas æ€§èƒ½æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨ pandas"
                else:
                    results["recommendation"] = "ä¸¤ç§æ–¹æ³•æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ®éœ€è¦é€‰æ‹©"
        
        elif operation == "write":
            if not test_data:
                return {
                    "success": False,
                    "error": "å†™å…¥æµ‹è¯•éœ€è¦æä¾› test_data å‚æ•°",
                    "suggestion": "è¯·æä¾›æµ‹è¯•æ•°æ®åˆ—è¡¨"
                }
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œæµ‹è¯•
            import tempfile
            
            with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp1, \
                 tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp2:
                
                # æµ‹è¯• Go æœåŠ¡æ€§èƒ½
                start_time = time.time()
                go_result = processor.write_excel_enhanced(tmp1.name, test_data, use_go=True)
                go_time = time.time() - start_time
                
                # æµ‹è¯• pandas æ€§èƒ½
                start_time = time.time()
                pandas_result = processor.write_excel_enhanced(tmp2.name, test_data, use_go=False)
                pandas_time = time.time() - start_time
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(tmp1.name)
                    os.unlink(tmp2.name)
                except:
                    pass
                
                results["performance_comparison"] = {
                    "go_service": {
                        "time_seconds": round(go_time, 4),
                        "success": go_result.get("success", False),
                        "method": go_result.get("data", {}).get("method", "unknown")
                    },
                    "pandas": {
                        "time_seconds": round(pandas_time, 4),
                        "success": pandas_result.get("success", False),
                        "method": pandas_result.get("data", {}).get("method", "unknown")
                    }
                }
                
                # æ€§èƒ½æå‡è®¡ç®—
                if go_time > 0 and pandas_time > 0:
                    speedup = pandas_time / go_time
                    results["performance_comparison"]["speedup"] = round(speedup, 2)
                    
                    if speedup > 2:
                        results["recommendation"] = f"Go æœåŠ¡æ¯” pandas å¿« {speedup:.1f} å€ï¼Œå»ºè®®ä½¿ç”¨ Go æœåŠ¡"
                    elif speedup < 0.8:
                        results["recommendation"] = "pandas æ€§èƒ½æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨ pandas"
                    else:
                        results["recommendation"] = "ä¸¤ç§æ–¹æ³•æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ®éœ€è¦é€‰æ‹©"
        
        else:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}",
                "suggestion": "æ”¯æŒçš„æ“ä½œç±»å‹: 'read', 'write'"
            }
        
        return results
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {str(e)}",
            "suggestion": "è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå‚æ•°è®¾ç½®"
        }


# æ³¨å†Œ Excel å…¬å¼å¤„ç†å·¥å…·
@mcp.tool()
def parse_formula(formula: str, validate_security: bool = False) -> str:
    """è§£æ Excel å…¬å¼
    
    Args:
        formula: Excel å…¬å¼å­—ç¬¦ä¸²
        validate_security: æ˜¯å¦è¿›è¡Œå®‰å…¨éªŒè¯
        
    Returns:
        str: JSON æ ¼å¼çš„è§£æç»“æœ
    """
    return parse_excel_formula(formula, validate_security)

@mcp.tool()
def compile_workbook(file_path: str, output_format: str = 'python') -> str:
    """ç¼–è¯‘ Excel å·¥ä½œç°¿ä¸ºä»£ç 
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        output_format: è¾“å‡ºæ ¼å¼ ('python' æˆ– 'json')
        
    Returns:
        str: JSON æ ¼å¼çš„ç¼–è¯‘ç»“æœ
    """
    return compile_excel_workbook(file_path, output_format)

@mcp.tool()
def execute_formula(formula: str, context: str = '{}') -> str:
    """æ‰§è¡Œ Excel å…¬å¼
    
    Args:
        formula: Excel å…¬å¼å­—ç¬¦ä¸²
        context: JSON æ ¼å¼çš„ä¸Šä¸‹æ–‡æ•°æ®
        
    Returns:
        str: JSON æ ¼å¼çš„æ‰§è¡Œç»“æœ
    """
    return execute_excel_formula(formula, context)

@mcp.tool()
def analyze_dependencies(file_path: str) -> str:
    """åˆ†æ Excel æ–‡ä»¶çš„å…¬å¼ä¾èµ–å…³ç³»
    
    Args:
        file_path: Excel æ–‡ä»¶è·¯å¾„
        
    Returns:
        str: JSON æ ¼å¼çš„ä¾èµ–åˆ†æç»“æœ
    """
    return analyze_excel_dependencies(file_path)

@mcp.tool()
def validate_formula(formula: str) -> str:
    """éªŒè¯ Excel å…¬å¼çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§
    
    Args:
        formula: Excel å…¬å¼å­—ç¬¦ä¸²
        
    Returns:
        str: JSON æ ¼å¼çš„éªŒè¯ç»“æœ
    """
    return validate_excel_formula(formula)


# å¢å¼ºçš„Excelæ•°æ®è´¨é‡æ§åˆ¶å·¥å…·
@mcp.tool()
def enhanced_data_quality_check(file_path: str, quality_level: str = "comprehensive") -> dict:
    """å¢å¼ºçš„Excelæ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        quality_level: è´¨é‡æ£€æŸ¥çº§åˆ« ("basic", "standard", "comprehensive")
        
    Returns:
        dict: æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ
    """
    try:
        return data_quality_controller.comprehensive_quality_check(file_path, quality_level)
    except Exception as e:
        return create_error_response("QUALITY_CHECK_ERROR", str(e))


@mcp.tool()
def extract_cell_content_advanced(file_path: str, cell_range: Optional[str] = None, sheet_name: Optional[str] = None, 
                                 extract_type: str = "all") -> dict:
    """é«˜çº§å•å…ƒæ ¼å†…å®¹æå–å·¥å…·
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        cell_range: å•å…ƒæ ¼èŒƒå›´ (å¦‚ "A1:C10")
        sheet_name: å·¥ä½œè¡¨åç§°
        extract_type: æå–ç±»å‹ ("all", "text", "numbers", "formulas", "formatted")
        
    Returns:
        dict: æå–çš„å•å…ƒæ ¼å†…å®¹
    """
    try:
        return cell_content_extractor.extract_cell_content_advanced(
            file_path, cell_range, sheet_name, extract_type
        )
    except Exception as e:
        return create_error_response("CELL_EXTRACTION_ERROR", str(e))


@mcp.tool()
def convert_character_formats(file_path: str, conversion_rules: dict, output_path: Optional[str] = None) -> dict:
    """å­—ç¬¦æ ¼å¼è‡ªåŠ¨åŒ–è½¬æ¢å·¥å…·
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        conversion_rules: è½¬æ¢è§„åˆ™å­—å…¸
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: è½¬æ¢ç»“æœ
    """
    try:
        return character_converter.batch_character_conversion(
            file_path, conversion_rules, output_path
        )
    except Exception as e:
        return create_error_response("CHARACTER_CONVERSION_ERROR", str(e))


@mcp.tool()
def extract_multi_condition_data(file_path: str, conditions: list, sheet_name: Optional[str] = None) -> dict:
    """å¤šæ¡ä»¶æ•°æ®æå–å·¥å…·
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        conditions: æ¡ä»¶åˆ—è¡¨
        sheet_name: å·¥ä½œè¡¨åç§°
        
    Returns:
        dict: æå–çš„æ•°æ®
    """
    try:
        return multi_condition_extractor.extract_with_multiple_conditions(
            file_path, conditions, sheet_name
        )
    except Exception as e:
        return create_error_response("MULTI_CONDITION_EXTRACTION_ERROR", str(e))


@mcp.tool()
def merge_multiple_tables(file_paths: list, merge_config: dict, output_path: Optional[str] = None) -> dict:
    """å¤šè¡¨æ ¼æ•°æ®åˆå¹¶å·¥å…·
    
    Args:
        file_paths: Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        merge_config: åˆå¹¶é…ç½®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åˆå¹¶ç»“æœ
    """
    try:
        return multi_table_merger.merge_multiple_excel_files(
            file_paths, merge_config, output_path
        )
    except Exception as e:
        return create_error_response("TABLE_MERGE_ERROR", str(e))


@mcp.tool()
def clean_excel_data(file_path: str, cleaning_options: dict, output_path: Optional[str] = None) -> dict:
    """Excelæ•°æ®æ¸…æ´—å·¥å…·
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        cleaning_options: æ¸…æ´—é€‰é¡¹
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: æ¸…æ´—ç»“æœ
    """
    try:
        return data_cleaner.clean_excel_data(
            file_path=file_path, 
            cleaning_config=cleaning_options, 
            output_file=output_path
        )
    except Exception as e:
        return create_error_response("DATA_CLEANING_ERROR", str(e))


@mcp.tool()
def batch_process_excel_files(file_paths: list, processing_config: dict) -> dict:
    """æ‰¹é‡Excelæ–‡ä»¶å¤„ç†å·¥å…·
    
    Args:
        file_paths: Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        processing_config: å¤„ç†é…ç½®
        
    Returns:
        dict: æ‰¹é‡å¤„ç†ç»“æœ
    """
    try:
        return batch_processor.batch_process_files(
            file_paths, processing_config
        )
    except Exception as e:
        return create_error_response("BATCH_PROCESSING_ERROR", str(e))


if __name__ == "__main__":
    # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
    os.makedirs(CHARTS_DIR, exist_ok=True)
    mcp.run()

